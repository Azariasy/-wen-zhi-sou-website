import argparse
from pathlib import Path
import os
import shutil
import docx
import jieba
import json
import re
import time
import zipfile
import rarfile
import tempfile
import io
import sys
import multiprocessing
import traceback
import threading
import pandas as pd
import markdown
import math
from html.parser import HTMLParser
from bs4 import BeautifulSoup
from striprtf.striprtf import rtf_to_text
import email
from email.parser import BytesParser
from email.header import decode_header
from email.utils import parseaddr
import chardet
import extract_msg
import csv
from datetime import datetime
import functools
# --- ADDED: å¯¼å…¥å¹¶å‘å¤„ç†æ¨¡å— ---
import asyncio
import concurrent.futures
from threading import Lock
import time
# ------------------------------------

# --- ADDED: è®¸å¯è¯ç®¡ç†å™¨æ”¯æŒ ---
try:
    from license_manager import get_license_manager, Features
    _license_manager_available = True
except ImportError:
    _license_manager_available = False

# --- å¯¼å…¥ç»Ÿä¸€è·¯å¾„å¤„ç†å·¥å…· ---
from path_utils import normalize_path_for_index, PathStandardizer

# --- å¯¼å…¥ç»Ÿä¸€æ–‡ä»¶å¤„ç†å·¥å…· ---
from file_processing_utils import check_cancellation, periodic_cancellation_check, InterruptedError

# -------------------------------

# è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
def is_feature_available(feature_name):
    """
    æ£€æŸ¥ç‰¹å®šåŠŸèƒ½æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœè®¸å¯è¯ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè¿”å› True
    
    Args:
        feature_name: è¦æ£€æŸ¥çš„åŠŸèƒ½åç§°
        
    Returns:
        bool: å¦‚æœåŠŸèƒ½å¯ç”¨è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if not _license_manager_available:
        return True  # å¦‚æœè®¸å¯è¯ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œé»˜è®¤æ‰€æœ‰åŠŸèƒ½å¯ç”¨
    
    try:
        license_manager = get_license_manager()
        return license_manager.is_feature_available(feature_name)
    except Exception as e:
        print(f"è®¸å¯è¯æ£€æŸ¥é”™è¯¯: {e}")
        return True  # å‘ç”Ÿé”™è¯¯æ—¶é»˜è®¤åŠŸèƒ½å¯ç”¨
# --------------------------------

from whoosh.index import create_in, open_dir, exists_in
from whoosh.fields import Schema, TEXT, ID, STORED, NUMERIC, KEYWORD
from whoosh.analysis import Tokenizer, Token, Analyzer
from whoosh.query import Phrase, Term, Prefix, And, Or, Not, Query, NumericRange, Every, FuzzyTerm, Wildcard, TermRange
from whoosh import scoring
from whoosh.qparser import QueryParser, MultifieldParser, OrGroup, GtLtPlugin, PhrasePlugin, SequencePlugin
from PyPDF2 import PdfReader
import pptx
import openpyxl
from datetime import datetime
import csv  # æ·»åŠ ç”¨äºå†™å…¥TSVæ–‡ä»¶
from tqdm import tqdm  # Added for better progress reporting
from whoosh.highlight import Highlighter, ContextFragmenter, HtmlFormatter
# --- ADDED: Import analysis module ---
from whoosh import analysis
# ------------------------------------

# --- ADDED: Imports for OCR --- 
import pytesseract
from PIL import Image
from pdf2image import convert_from_path, pdfinfo_from_path
from pytesseract import TesseractError # Added
import pdf2image
from pdf2image.exceptions import PDFPopplerTimeoutError, PDFPageCountError # Added exceptions
# ------------------------------

# --- Constants ---
# INDEX_DIR = "indexdir" # Commented out, will be passed as parameter
ALLOWED_EXTENSIONS = [
    # æ–‡æ¡£ç±»å‹
    '.txt', '.docx', '.pdf', '.zip', '.rar', '.pptx', '.xlsx', '.md', '.html', '.htm', '.rtf', '.eml', '.msg',
    # è§†é¢‘æ–‡ä»¶ (ä»…æ–‡ä»¶åæœç´¢)
    '.mp4', '.mkv', '.avi', '.wmv', '.mov', '.flv', '.webm', '.m4v',
    # éŸ³é¢‘æ–‡ä»¶ (ä»…æ–‡ä»¶åæœç´¢)
    '.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a',
    # å›¾ç‰‡æ–‡ä»¶ (ä»…æ–‡ä»¶åæœç´¢)
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg'
]

# ä»…è¿›è¡Œæ–‡ä»¶åç´¢å¼•çš„æ–‡ä»¶ç±»å‹ï¼ˆä¸æå–å†…å®¹ï¼‰
FILENAME_ONLY_EXTENSIONS = {
    '.mp4', '.mkv', '.avi', '.wmv', '.mov', '.flv', '.webm', '.m4v',  # è§†é¢‘
    '.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a',         # éŸ³é¢‘
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg' # å›¾ç‰‡
}

# --- æ–°å¢å‡½æ•°ç”¨äºè®°å½•è·³è¿‡æ–‡ä»¶çš„ä¿¡æ¯ ---
def record_skipped_file(index_dir_path: str, file_path: str, reason: str) -> None:
    """
    è®°å½•è¢«è·³è¿‡çš„æ–‡ä»¶ä¿¡æ¯åˆ°TSVæ–‡ä»¶ä¸­ã€‚
    
    Args:
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        file_path: è¢«è·³è¿‡çš„æ–‡ä»¶è·¯å¾„
        reason: è·³è¿‡åŸå› 
    """
    try:
        # æ„å»ºè®°å½•æ–‡ä»¶è·¯å¾„
        log_file_path = os.path.join(index_dir_path, "index_skipped_files.tsv")
        
        # è·å–å½“å‰æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ è¡¨å¤´
        file_exists = os.path.isfile(log_file_path)
        
        # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶
        with open(log_file_path, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f, delimiter='\t')
            
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ·»åŠ è¡¨å¤´
            if not file_exists:
                writer.writerow(["æ–‡ä»¶è·¯å¾„", "è·³è¿‡åŸå› ", "æ—¶é—´"])
            
            # å†™å…¥è®°å½•
            writer.writerow([file_path, reason, timestamp])
            
    except Exception as e:
        # å¦‚æœè®°å½•è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œæ‰“å°é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
        print(f"Warning: Failed to record skipped file {file_path}: {e}", file=sys.stderr)
        pass

# --- ç”¨äºå¤„ç†è·³è¿‡åŸå› çš„æ ¼å¼åŒ–å‡½æ•° ---
def format_skip_reason(reason_type: str, detail: str = "") -> str:
    """
    æ ¼å¼åŒ–è·³è¿‡æ–‡ä»¶çš„åŸå› ï¼Œä½¿å…¶æ›´æ˜“äºç†è§£ã€‚
    
    Args:
        reason_type: è·³è¿‡ç±»å‹
        detail: è¯¦ç»†ä¿¡æ¯
    
    Returns:
        æ ¼å¼åŒ–åçš„åŸå› è¯´æ˜
    """
    reason_map = {
        "password_zip": "éœ€è¦å¯†ç çš„ZIPæ–‡ä»¶",
        "password_rar": "éœ€è¦å¯†ç çš„RARæ–‡ä»¶",
        "corrupted_zip": "æŸåçš„ZIPæ–‡ä»¶",
        "corrupted_rar": "æŸåçš„RARæ–‡ä»¶",
        "ocr_timeout": "OCRå¤„ç†è¶…æ—¶",
        "pdf_conversion_timeout": "PDFè½¬æ¢è¶…æ—¶",
        "content_too_large": "å†…å®¹è¶…å‡ºå¤§å°é™åˆ¶",
        "unsupported_type": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹",
        "extraction_error": "æå–è¿‡ç¨‹å‡ºé”™",
        "missing_dependency": "ç¼ºå°‘å¤„ç†ä¾èµ–",
        "pdf_timeout": "PDFå¤„ç†è¶…æ—¶æˆ–è½¬æ¢é”™è¯¯",
        "content_limit": "å†…å®¹å¤§å°è¶…è¿‡é™åˆ¶",
    }
    
    base_reason = reason_map.get(reason_type, "æœªçŸ¥åŸå› ")
    
    if detail:
        return f"{base_reason} - {detail}"
    return base_reason
# -----------------------------------

# --- ADDED: Configuration for Tesseract and Poppler --- 
# Set the path to the tesseract executable if it's not in PATH
_TESSERACT_CMD = r'C:\Program Files\Tesseract-OCR\tesseract.exe' # Use double backslashes or raw string
_tesseract_found = False
if os.path.exists(_TESSERACT_CMD):
     pytesseract.pytesseract.tesseract_cmd = _TESSERACT_CMD
     _tesseract_found = True
else:
     if shutil.which('tesseract'):
         # print("Tesseract executable found in PATH.") # COMMENTED OUT
         _tesseract_found = True
     else:
        # print(f"Warning: Tesseract executable not found at {_TESSERACT_CMD} or in PATH. OCR will likely fail.") # COMMENTED OUT
        pass

# Path to the Poppler bin directory
_POPPLER_PATH = r"C:\Program Files\poppler-24.08.0\Library\bin" # Use the path from user screenshot
_poppler_found = False
if os.path.isdir(_POPPLER_PATH) and os.path.exists(os.path.join(_POPPLER_PATH, 'pdfinfo.exe')):
    _poppler_found = True
else:
    # print(f"Warning: Poppler bin directory not found or pdfinfo.exe missing at {_POPPLER_PATH}. PDF to image conversion will likely fail.") # COMMENTED OUT
    pass
# ------------------------------------------------------

# --- Global Index Lock ---
INDEX_ACCESS_LOCK = threading.Lock()

# --- Custom Jieba Tokenizer and Analyzer ---
class ChineseTokenizer(Tokenizer):
    def __call__(self, value, positions=False, chars=False,
                 keeporiginal=False, removestops=True,
                 start_pos=0, start_char=0,
                 mode='', **kwargs):
        assert isinstance(value, str), "ChineseTokenizer expects unicode string"
        
        # --- ADDED: æ£€æµ‹æ˜¯å¦ä¸ºç²¾ç¡®æœç´¢æ¨¡å¼ ---
        is_phrase_mode = kwargs.get('phrase_mode', False)
        
        if is_phrase_mode:
            # ç²¾ç¡®æœç´¢æ¨¡å¼ï¼šä¿æŒåŸå§‹å­—ç¬¦ä¸²çš„å®Œæ•´æ€§ï¼Œåªè¿›è¡Œæœ€å°åˆ†è¯
            # å¯¹äºåŒ…å«ç©ºæ ¼çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†
            if ' ' in value.strip():
                # åŒ…å«ç©ºæ ¼çš„æŸ¥è¯¢ï¼šå°è¯•ä¿æŒç©ºæ ¼å‰åçš„è¯ç»„å®Œæ•´æ€§
                # ä¾‹å¦‚ï¼š"éƒ¨é—¨ A" åº”è¯¥è¢«å¤„ç†ä¸ºä¸€ä¸ªæ•´ä½“æˆ–åˆç†çš„åˆ†æ®µ
                
                # æ–¹æ¡ˆ1ï¼šå°†æ•´ä¸ªå­—ç¬¦ä¸²ä½œä¸ºä¸€ä¸ªtokenï¼ˆé€‚ç”¨äºçŸ­æŸ¥è¯¢ï¼‰
                if len(value.strip()) <= 20:  # çŸ­æŸ¥è¯¢ç›´æ¥ä½œä¸ºæ•´ä½“
                    token = Token(positions=positions, chars=chars, removestops=removestops, mode=mode, **kwargs)
                    token.text = value.strip()
                    token.boost = 1.0
                    if keeporiginal:
                        token.original = value.strip()
                    token.stopped = False
                    if positions:
                        token.pos = 0
                    if chars:
                        token.startchar = start_char
                        token.endchar = start_char + len(value.strip())
                    yield token
                    return
                
                # æ–¹æ¡ˆ2ï¼šæ™ºèƒ½åˆ†æ®µï¼Œä¿æŒæœ‰æ„ä¹‰çš„è¯ç»„
                # æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œä½†ä¿æŒæ¯ä¸ªéƒ¨åˆ†çš„å®Œæ•´æ€§
                parts = [part.strip() for part in value.split() if part.strip()]
                token_pos = 0
                current_char = start_char
                
                for part in parts:
                    # å¯¹æ¯ä¸ªéƒ¨åˆ†è¿›è¡Œjiebaåˆ†è¯
                    seglist = jieba.tokenize(part)
                    for (word, start, end) in seglist:
                        if word.strip():  # è·³è¿‡çº¯ç©ºæ ¼token
                            token = Token(positions=positions, chars=chars, removestops=removestops, mode=mode, **kwargs)
                            token.text = word
                            token.boost = 1.0
                            if keeporiginal:
                                token.original = word
                            token.stopped = False
                            if positions:
                                token.pos = token_pos
                            if chars:
                                token.startchar = current_char + start
                                token.endchar = current_char + end
                            yield token
                            token_pos += 1
                    current_char += len(part) + 1  # +1 for space
                return
        
        # --- åŸå§‹é€»è¾‘ï¼šç”¨äºæ¨¡ç³Šæœç´¢å’Œç´¢å¼• ---
        seglist = jieba.tokenize(value)
        token_pos = 0
        for (word, start, end) in seglist:
            # --- MODIFIED: åœ¨éç²¾ç¡®æ¨¡å¼ä¸‹è·³è¿‡çº¯ç©ºæ ¼token ---
            if not is_phrase_mode and word.strip() == '':
                continue
                
            token = Token(positions=positions, chars=chars, removestops=removestops, mode=mode, **kwargs)
            token.text = word
            token.boost = 1.0
            if keeporiginal:
                token.original = word
            token.stopped = False
            if positions:
                token.pos = token_pos
            if chars:
                token.startchar = start_char + start
                token.endchar = start_char + end
            yield token
            token_pos += 1

class ChineseAnalyzer(Analyzer):
    def __call__(self, value, **kwargs):
        # --- FIXED: æ­£ç¡®ä¼ é€’kwargså‚æ•°åˆ°ChineseTokenizer ---
        tokenizer = ChineseTokenizer()
        return tokenizer(value, **kwargs)

# --- HTML Stripper ---
class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.text = io.StringIO()
    def handle_data(self, d):
        self.text.write(d)
    def get_data(self):
        return self.text.getvalue()

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

# --- ADDED: å¹¶è¡Œæœç´¢å¼•æ“ä¼˜åŒ–ç±» ---
class OptimizedSearchEngine:
    """ä¼˜åŒ–çš„å¹¶è¡Œæœç´¢å¼•æ“"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.search_lock = Lock()
        self.result_cache = {}
        self.cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜è¿‡æœŸ
        
    def _get_cache_key(self, query_str: str, search_params: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_data = f"{query_str}_{sorted(search_params.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
        
    def _is_cache_valid(self, cache_entry: dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        return time.time() - cache_entry['timestamp'] < self.cache_timeout
        
    def _analyze_query_complexity(self, query_str: str, search_params: dict) -> str:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        # ç®€å•æŸ¥è¯¢ï¼šçŸ­æ–‡æœ¬ï¼Œæ— é€šé…ç¬¦ï¼Œæ— å¤æ‚è¿‡æ»¤
        if (len(query_str) <= 10 and 
            '*' not in query_str and '?' not in query_str and
            not search_params.get('file_type_filter') and
            not search_params.get('min_size_kb') and
            not search_params.get('start_date')):
            return 'simple'
            
        # å¤æ‚æŸ¥è¯¢ï¼šå¤šä¸ªé€šé…ç¬¦ï¼Œå¤šä¸ªè¿‡æ»¤æ¡ä»¶
        wildcard_count = query_str.count('*') + query_str.count('?')
        filter_count = sum(1 for k in ['file_type_filter', 'min_size_kb', 'max_size_kb', 'start_date', 'end_date'] 
                          if search_params.get(k))
        
        if wildcard_count > 2 or filter_count > 2:
            return 'complex'
            
        return 'medium'
        
    async def optimized_search(self, query_str: str, index_dir_path: str, **search_params) -> list[dict]:
        """ä¼˜åŒ–çš„æœç´¢å…¥å£"""
        start_time = time.time()
        
        # ç§»é™¤ä¸å…¼å®¹çš„å‚æ•°
        clean_params = search_params.copy()
        if 'limit' in clean_params:
            del clean_params['limit']
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(query_str, clean_params)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.result_cache:
            cache_entry = self.result_cache[cache_key]
            if self._is_cache_valid(cache_entry):
                print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {query_str} ({len(cache_entry['results'])} ç»“æœ)")
                return cache_entry['results']
            else:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                del self.result_cache[cache_key]
                
        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
        complexity = self._analyze_query_complexity(query_str, clean_params)
        print(f"ğŸ” æŸ¥è¯¢å¤æ‚åº¦: {complexity}")
        
        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©æœç´¢ç­–ç•¥
        if complexity == 'simple':
            results = await self._fast_simple_search(query_str, index_dir_path, **clean_params)
        elif complexity == 'medium':
            results = await self._parallel_search(query_str, index_dir_path, **clean_params)
        else:
            results = await self._complex_search_with_optimization(query_str, index_dir_path, **clean_params)
            
        # ç¼“å­˜ç»“æœ
        self.result_cache[cache_key] = {
            'results': results,
            'timestamp': time.time()
        }
        
        search_time = time.time() - start_time
        print(f"âš¡ ä¼˜åŒ–æœç´¢å®Œæˆ: {search_time:.2f}ç§’, {len(results)} ç»“æœ")
        
        return results
        
    async def _fast_simple_search(self, query_str: str, index_dir_path: str, **search_params) -> list[dict]:
        """å¿«é€Ÿç®€å•æœç´¢"""
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            results = await loop.run_in_executor(
                executor, 
                lambda: search_index(query_str, index_dir_path, **search_params)
            )
        # å¯¹äºç®€å•æŸ¥è¯¢ï¼Œé™åˆ¶è¿”å›ç»“æœæ•°ï¼ˆé™åˆ¶åˆ°500æ¡ï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰
        return results[:500] if len(results) > 500 else results
        
    async def _parallel_search(self, query_str: str, index_dir_path: str, **search_params) -> list[dict]:
        """å¹¶è¡Œæœç´¢ï¼ˆé€‚ç”¨äºä¸­ç­‰å¤æ‚åº¦æŸ¥è¯¢ï¼‰"""
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºå¤šä¸ªæœç´¢ä»»åŠ¡
        tasks = []
        
        # å¦‚æœæœ‰æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼Œå¯ä»¥åˆ†åˆ«æœç´¢ä¸åŒç±»å‹
        file_types = search_params.get('file_type_filter')
        if file_types and len(file_types) > 1:
            # åˆ†æ–‡ä»¶ç±»å‹å¹¶è¡Œæœç´¢
            for file_type in file_types:
                task_params = search_params.copy()
                task_params['file_type_filter'] = [file_type]
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    task = loop.run_in_executor(
                        executor,
                        lambda ft=file_type, tp=task_params: search_index(query_str, index_dir_path, **tp)
                    )
                    tasks.append(task)
                    
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶åˆå¹¶ç»“æœ
            if tasks:
                all_results = await asyncio.gather(*tasks)
                merged_results = []
                seen_paths = set()
                
                for results in all_results:
                    for result in results:
                        path = result.get('file_path')
                        if path not in seen_paths:
                            merged_results.append(result)
                            seen_paths.add(path)
                            
                # æŒ‰ç›¸å…³åº¦é‡æ–°æ’åº
                merged_results.sort(key=lambda x: x.get('score', 0), reverse=True)
                return merged_results[:500]  # é™åˆ¶æœ€å¤šè¿”å›500ä¸ªç»“æœ
        
        # å¦‚æœæ— æ³•å¹¶è¡ŒåŒ–ï¼Œä½¿ç”¨å•çº¿ç¨‹æœç´¢
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            results = await loop.run_in_executor(
                executor,
                lambda: search_index(query_str, index_dir_path, **search_params)
            )
        return results
        
    async def _complex_search_with_optimization(self, query_str: str, index_dir_path: str, **search_params) -> list[dict]:
        """å¤æ‚æœç´¢ä¼˜åŒ–"""
        # å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µæœç´¢ç­–ç•¥
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæ–‡ä»¶åæœç´¢
        filename_params = search_params.copy()
        filename_params['search_scope'] = 'filename'
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            filename_results = await loop.run_in_executor(
                executor,
                lambda: search_index(query_str, index_dir_path, **filename_params)
            )
        
        print(f"ğŸ“ æ–‡ä»¶åæœç´¢: {len(filename_results)} ç»“æœ")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå…¨æ–‡æœç´¢
        fulltext_params = search_params.copy()
        fulltext_params['search_scope'] = 'fulltext'
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            fulltext_results = await loop.run_in_executor(
                executor,
                lambda: search_index(query_str, index_dir_path, **fulltext_params)
            )
            
        print(f"ğŸ“„ å…¨æ–‡æœç´¢: {len(fulltext_results)} ç»“æœ")
        
        # åˆå¹¶ç»“æœï¼Œå»é‡ï¼ŒæŒ‰ç›¸å…³åº¦æ’åº
        all_results = filename_results + fulltext_results
        seen_paths = set()
        merged_results = []
        
        for result in all_results:
            path = result.get('file_path')
            if path not in seen_paths:
                merged_results.append(result)
                seen_paths.add(path)
                
        # æŒ‰ç›¸å…³åº¦æ’åº
        merged_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        return merged_results[:500]  # é™åˆ¶æœ€å¤šè¿”å›500ä¸ªç»“æœ
        
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.result_cache.clear()
        print("ğŸ§¹ æœç´¢ç¼“å­˜å·²æ¸…ç†")
        
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        valid_entries = sum(1 for entry in self.result_cache.values() 
                           if self._is_cache_valid(entry))
        return {
            'total_entries': len(self.result_cache),
            'valid_entries': valid_entries,
            'cache_hit_potential': valid_entries / max(len(self.result_cache), 1)
        }

# åˆ›å»ºå…¨å±€ä¼˜åŒ–æœç´¢å¼•æ“å®ä¾‹
_optimized_search_engine = None

def get_optimized_search_engine() -> OptimizedSearchEngine:
    """è·å–å…¨å±€ä¼˜åŒ–æœç´¢å¼•æ“å®ä¾‹"""
    global _optimized_search_engine
    if _optimized_search_engine is None:
        _optimized_search_engine = OptimizedSearchEngine(max_workers=4)
    return _optimized_search_engine

def optimized_search_sync(query_str: str, index_dir_path: str, **search_params) -> list[dict]:
    """åŒæ­¥ç‰ˆæœ¬çš„ä¼˜åŒ–æœç´¢æ¥å£"""
    engine = get_optimized_search_engine()
    
    # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œç›´æ¥è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬
    try:
        loop = asyncio.get_running_loop()
        # åœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­ï¼Œåˆ›å»ºæ–°ä»»åŠ¡
        task = asyncio.create_task(engine.optimized_search(query_str, index_dir_path, **search_params))
        return asyncio.run_coroutine_threadsafe(task, loop).result()
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
        return asyncio.run(engine.optimized_search(query_str, index_dir_path, **search_params))
# ------------------------------------

def get_schema() -> Schema:
    analyzer = ChineseAnalyzer()
    return Schema(path=ID(stored=True, unique=True),
                  content=TEXT(stored=True, analyzer=analyzer),
                  # --- ADDED: Field for filename search ---
                  filename_text=TEXT(stored=True, analyzer=analysis.StandardAnalyzer()),
                  # ---------------------------------------------------
                  structure_map=STORED,
                  last_modified=NUMERIC(stored=True, sortable=True),
                  file_size=NUMERIC(stored=True, sortable=True),
                  file_type=KEYWORD(stored=True, lowercase=True, scorable=False),
                  # --- ADDED: Field to store OCR indexing status ---
                  indexed_with_ocr=STORED)
                  # -------------------------------------------------

def scan_documents(directory_path: Path) -> list[Path]:
    found_files = []
    if not directory_path.is_dir():
        # print(f"Error: Path is not a directory: {directory_path}") # COMMENTED OUT
        return found_files
    # print(f"Scanning directory: {directory_path}") # COMMENTED OUT
    for item in directory_path.rglob('*'):
        if item.is_file() and item.suffix.lower() in ALLOWED_EXTENSIONS:
            found_files.append(item)
    # print(f"Scan complete. Found {len(found_files)} document(s).") # COMMENTED OUT
    return found_files

def extract_text_from_docx(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    full_text_list = []
    structure = []
    try:
        # å¼€å§‹å¤„ç†å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "DOCXæ–‡ä»¶å¤„ç†")
        
        doc = docx.Document(file_path)
        for i, para in enumerate(doc.paragraphs):
            # æ¯å¤„ç†50ä¸ªæ®µè½æ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€ï¼ˆä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°ï¼‰
            periodic_cancellation_check(cancel_callback, 50, i, "DOCXæ®µè½å¤„ç†")
                
            text = para.text.strip()
            if not text:
                continue
            full_text_list.append(text)
            para_info = {"text": text}
            style_name = para.style.name.lower()
            if style_name.startswith('heading'):
                para_info["type"] = "heading"
                try:
                    level = int(style_name.split()[-1])
                    para_info["level"] = level
                except (ValueError, IndexError):
                    para_info["level"] = 1
            else:
                para_info["type"] = "paragraph"
            structure.append(para_info)
        
        # å¤„ç†å®Œæˆå‰æœ€åæ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "DOCXæ–‡ä»¶å¤„ç†")
        
        full_text = '\n'.join(full_text_list)
        return full_text, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except Exception as e:
        # print(f"Error reading docx file {file_path}: {e}") # COMMENTED OUT
        return "", []

def extract_text_from_txt(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    content = ""
    structure = []
    encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'latin-1']
    heading_pattern = re.compile(
        r'^\s*(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡]+[ç« æ¡]|'  
        r'[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|'             
        r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡]+[)ï¼‰]|' 
        r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]|' 
        r'\d+(?:\.\d+)*\.?\s+|'                
        r'ç¬¬.*?èŠ‚)\s*$',                         
        re.IGNORECASE
    )
    try:
        for encoding in encodings_to_try:
            try:
                # åœ¨å¼€å§‹è¯»å–æ–‡ä»¶å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
                check_cancellation(cancel_callback, "TXTæ–‡ä»¶è¯»å–")
                    
                content = file_path.read_text(encoding=encoding)
                if content:
                    lines = content.splitlines()
                    for i, line in enumerate(lines):
                        # å¯¹äºå¤§æ–‡ä»¶ï¼Œæ¯å¤„ç†100è¡Œæ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€
                        periodic_cancellation_check(cancel_callback, 100, i, "TXTæ–‡ä»¶è¡Œå¤„ç†")
                            
                        cleaned_line = line.strip()
                        if cleaned_line:
                            if heading_pattern.match(cleaned_line):
                                structure.append({'type': 'heading', 'level': 1, 'text': cleaned_line})
                            else:
                                structure.append({'type': 'paragraph', 'text': cleaned_line})
                if content or structure:
                    break
            except InterruptedError:
                # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                raise
            except UnicodeDecodeError:
                continue
            except Exception as e:
                # print(f"Error reading txt file {file_path} with encoding {encoding}: {e}") # COMMENTED OUT
                return "", []
        if not content and not structure:
            print(f"Could not decode txt file {file_path} with tried encodings or file is empty.")
            return "", []
    except Exception as e:
        print(f"Error accessing txt file {file_path}: {e}")
        return "", []
    return content, structure

def extract_text_from_pdf(file_path: Path, enable_ocr: bool = True, ocr_lang: str = 'chi_sim+eng', min_chars_for_ocr_trigger: int = 50, timeout: int | None = None, cancel_callback=None) -> tuple[str | None, list[dict]]:
    """
    ä»PDFæ–‡ä»¶æå–æ–‡æœ¬ï¼Œæ”¯æŒåŸºäºPyPDF2å’ŒOCRçš„æ··åˆæ–¹æ³•
    """
    # --- ADDED: æ£€æŸ¥PDFæ”¯æŒè®¸å¯è¯ ---
    if not is_feature_available(Features.PDF_SUPPORT):
        print(f"PDFæ”¯æŒåŠŸèƒ½ä¸å¯ç”¨ (æœªè·å¾—è®¸å¯)")
        raise PermissionError("PDFæ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯ã€‚")
    # -------------------------------
    
    # åŸºç¡€å¼‚å¸¸æ£€æŸ¥
    if not file_path.exists():
        print(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None, []
    
    # --- ADDED: æ—©æœŸå–æ¶ˆæ£€æŸ¥ ---
    check_cancellation(cancel_callback, "PDFæ–‡ä»¶å¤„ç†")
    # ---------------------------
    
    extracted_text = ""
    direct_text = ""
    structure = [] # Initialize structure list

    # 1. Try direct text extraction (using PyPDF2 or similar - currently placeholder)
    #    NOTE: Direct text extraction libraries usually don't have configurable timeouts easily.
    try:
        # --- Placeholder for direct text extraction ---
        # Example using PyPDF2 (if installed):
        # from PyPDF2 import PdfReader
        # reader = PdfReader(file_path)
        # for page in reader.pages:
        #     page_content = page.extract_text()
        #     if page_content:
        #         direct_text += page_content + "\n"
        # direct_text = direct_text.strip()
        # --- End Placeholder ---

        if not enable_ocr:
            print(f"Warning: Direct PDF text extraction not fully implemented or OCR disabled for {file_path.name}. Relying on potentially empty direct_text.")
            pass
    except Exception as e:
        print(f"Error during direct text extraction attempt for {file_path.name}: {e}", file=sys.stderr)
        direct_text = ""

    ocr_needed = enable_ocr and (len(direct_text) < min_chars_for_ocr_trigger)

    if ocr_needed:
        print(f"Info: {file_path.name} direct text insufficient ({len(direct_text)} chars) or not attempted, trying OCR...")
        ocr_texts = []
        try:
            # --- ADDED: åœ¨å¼€å§‹OCRå‰å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
            check_cancellation(cancel_callback, "PDF OCRå¤„ç†å¼€å§‹")
            # ----------------------------------------
                
            # --- æ·»åŠ  pdf2image æ—¶é—´æ—¥å¿— ---
            pdf2image_start_time = time.time()
            print(f"DEBUG: [{file_path.name}] Starting pdf2image conversion at {pdf2image_start_time:.2f}")
            # -------------------------------

            # --- MODIFIED: ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡ŒPDFè½¬æ¢ï¼Œä»¥ä¾¿æ›´å¿«å“åº”å–æ¶ˆ ---
            # å°†åŸå§‹è¶…æ—¶æ—¶é—´åˆ†å‰²ä¸ºæ›´å°çš„å—ï¼Œæ¯ä¸ªå—åæ£€æŸ¥å–æ¶ˆçŠ¶æ€
            chunk_timeout = min(30, timeout) if timeout else 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

            images = pdf2image.convert_from_path(
                            file_path,
                timeout=chunk_timeout,  # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶
                fmt='jpeg',
                thread_count=1
            )

            # --- æ·»åŠ  pdf2image æ—¶é—´æ—¥å¿— ---
            pdf2image_end_time = time.time()
            pdf2image_duration = pdf2image_end_time - pdf2image_start_time
            print(f"DEBUG: [{file_path.name}] Finished pdf2image conversion at {pdf2image_end_time:.2f} (Duration: {pdf2image_duration:.2f}s)")
            # -------------------------------

            # --- ADDED: åœ¨å¼€å§‹å¤„ç†é¡µé¢å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
            check_cancellation(cancel_callback, "PDFé¡µé¢å¤„ç†å¼€å§‹")
            # ----------------------------------------

            for i, image in enumerate(images):
                # --- MODIFIED: åœ¨å¤„ç†æ¯ä¸ªé¡µé¢å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ ---
                check_cancellation(cancel_callback, f"PDFé¡µé¢ {i + 1} å¤„ç†")
                # ------------------------------------------------
                    
                page_num = i + 1
                try:
                    # --- æ·»åŠ  Tesseract æ—¶é—´æ—¥å¿— ---
                    tesseract_start_time = time.time()
                    print(f"DEBUG: [{file_path.name} Page {page_num}] Starting Tesseract OCR at {tesseract_start_time:.2f}")
                    # ------------------------------

                    # --- MODIFIED: ä½¿ç”¨æ›´çŸ­çš„OCRè¶…æ—¶æ—¶é—´ ---
                    ocr_timeout = min(60, timeout) if timeout else 60  # æ¯é¡µæœ€å¤š60ç§’
                    page_text = pytesseract.image_to_string(image, lang=ocr_lang, timeout=ocr_timeout)

                    # --- æ·»åŠ  Tesseract æ—¶é—´æ—¥å¿— ---
                    tesseract_end_time = time.time()
                    tesseract_duration = tesseract_end_time - tesseract_start_time
                    print(f"DEBUG: [{file_path.name} Page {page_num}] Finished Tesseract OCR at {tesseract_end_time:.2f} (Duration: {tesseract_duration:.2f}s)")
                    # ------------------------------

                    page_text = page_text.strip()
                    if page_text:
                        ocr_texts.append(page_text)
                    else:
                        pass

                    # --- ADDED: åœ¨æ¯é¡µOCRå®Œæˆåæ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
                    check_cancellation(cancel_callback, f"PDFé¡µé¢ {page_num} OCRå®Œæˆ")
                    # ----------------------------------------
                        
                except TesseractError as te:
                    tesseract_fail_time = time.time()
                    print(f"DEBUG: [{file_path.name} Page {page_num}] TesseractError at {tesseract_fail_time:.2f} (Duration before error: {tesseract_fail_time - tesseract_start_time:.2f}s)")
                    err_msg = str(te).lower()
                    if 'timeout' in err_msg or 'process timed out' in err_msg:
                        print(f"Warning: Tesseract OCR timed out (>{ocr_timeout}s) for page {page_num} of {file_path.name}.", file=sys.stderr)
                        # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
                        # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
                        return None, [] # MODIFIED
                    else:
                        print(f"Error during Tesseract OCR for page {page_num} of {file_path.name}: {te}", file=sys.stderr)
                        # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
                        # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
                        return None, [] # MODIFIED
                except RuntimeError as rte:
                    tesseract_fail_time = time.time()
                    print(f"DEBUG: [{file_path.name} Page {page_num}] RuntimeError at {tesseract_fail_time:.2f} (Duration before error: {tesseract_fail_time - tesseract_start_time:.2f}s)")
                    err_msg = str(rte).lower()
                    if 'timeout' in err_msg:
                        print(f"Warning: Tesseract OCR likely timed out (>{ocr_timeout}s) for page {page_num} of {file_path.name}. Error: {rte}", file=sys.stderr)
                        # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
                        # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
                        return None, [] # MODIFIED
                    else:
                        print(f"Runtime error during Tesseract OCR for page {page_num} of {file_path.name}: {rte}", file=sys.stderr)
                        # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
                        # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
                        return None, [] # MODIFIED
                except Exception as e:
                    tesseract_fail_time = time.time()
                    print(f"DEBUG: [{file_path.name} Page {page_num}] Exception at {tesseract_fail_time:.2f} (Duration before error: {tesseract_fail_time - tesseract_start_time:.2f}s)")
                    print(f"Unexpected error during OCR for page {page_num} of {file_path.name}: {e}", file=sys.stderr)
                    # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
                    # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
                    return None, [] # MODIFIED
                # --- End Modification ---

            extracted_text = "\n\n".join(ocr_texts)
            if images:
                 print(f"Info: OCR process completed for {file_path.name}. Total chars: {len(extracted_text)}")

        # --- MODIFIED: Return tuple on exceptions ---
        except PDFPopplerTimeoutError:
            pdf2image_fail_time = time.time()
            print(f"DEBUG: [{file_path.name}] PDFPopplerTimeoutError at {pdf2image_fail_time:.2f} (Duration before error: {pdf2image_fail_time - pdf2image_start_time:.2f}s)")
            print(f"Warning: PDF to image conversion timed out (>{chunk_timeout}s) for {file_path.name}.", file=sys.stderr)
            # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
            # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
            return None, [] # MODIFIED
        except PDFPageCountError as pe:
             print(f"Error getting page count or converting PDF {file_path.name}: {pe}. Skipping OCR.", file=sys.stderr)
             # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
             # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
             return None, [] # MODIFIED
        except InterruptedError:
            # --- ADDED: ä¸“é—¨å¤„ç†ç”¨æˆ·å–æ¶ˆ ---
            print(f"PDF OCR processing cancelled by user for {file_path.name}")
            raise  # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        except Exception as e:
            pdf2image_fail_time = time.time()
            duration_str = f"(Duration before error: {pdf2image_fail_time - pdf2image_start_time:.2f}s)" if 'pdf2image_start_time' in locals() else ""
            print(f"DEBUG: [{file_path.name}] Exception during PDF conversion at {pdf2image_fail_time:.2f} {duration_str}")
            print(f"Error during PDF processing/conversion for {file_path.name}: {e}", file=sys.stderr)
            print(traceback.format_exc(), file=sys.stderr)
            # è¿™é‡Œä¸è®°å½•è·³è¿‡çš„æ–‡ä»¶ï¼Œå› ä¸ºè¯¥å‡½æ•°æ— æ³•è®¿é—®index_dir_path
            # è®°å½•ä¼šåœ¨_extract_workerä¸­å®Œæˆ
            return None, [] # MODIFIED
        # --- End Modification ---

    #     else:
        extracted_text = direct_text
        # ... (logging) ...

    # --- Generate basic structure from final extracted_text ---
    # Note: If direct_text extraction were implemented, it might populate 'structure' directly.
    # This block ensures structure is generated if only 'extracted_text' is available (e.g., from OCR).
    if not structure and extracted_text: # Generate structure only if it's empty AND text exists
        lines = extracted_text.splitlines()
        for line in lines:
            cleaned_line = line.strip()
            if cleaned_line:
                structure.append({'type': 'paragraph', 'text': cleaned_line})
    # -----------------------------------------------------------

    # MODIFIED: Return tuple, handle potential None for text
    if extracted_text is None:
         return None, [] # Structure is already empty list in this case
    else:
         # Ensure structure is a list even if extraction yielded text but somehow failed structure generation
         final_structure = structure if isinstance(structure, list) else []
         return extracted_text, final_structure

def extract_text_from_pptx(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    full_text_list = []
    structure = []
    try:
        # å¼€å§‹å¤„ç†å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "PPTXæ–‡ä»¶å¤„ç†")
        
        presentation = pptx.Presentation(file_path)
        for slide_num, slide in enumerate(presentation.slides):
            # åœ¨å¤„ç†æ¯ä¸ªå¹»ç¯ç‰‡å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            check_cancellation(cancel_callback, f"PPTXå¹»ç¯ç‰‡ {slide_num + 1} å¤„ç†")
                
            slide_texts = []
            for shape in slide.shapes:
                if not shape.has_text_frame:
                    continue
                text = shape.text.strip()
                if text:
                    slide_texts.append(text)
                    structure.append({
                        'type': 'paragraph',
                        'text': text,
                        'context': f'Slide {slide_num + 1}'
                    })
            if slide_texts:
                full_text_list.extend(slide_texts)
        full_text = '\n'.join(full_text_list)
        return full_text, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except Exception as e:
        print(f"Error reading pptx file {file_path}: {e}")
        return "", []

def _is_potential_header(row_data: list, non_empty_threshold=0.5, string_threshold=0.5) -> bool:
    if not row_data:
        return False
    non_empty_count = sum(1 for cell in row_data if pd.notna(cell) and str(cell).strip())
    string_count = sum(1 for cell in row_data if isinstance(cell, str) and str(cell).strip() and not str(cell).replace('.', '', 1).isdigit())
    if non_empty_count == 0:
        return False
    return (non_empty_count / len(row_data) >= non_empty_threshold and
            string_count / non_empty_count >= string_threshold)

def extract_text_from_xlsx(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    full_text_list = []
    structure = []
    MAX_HEADER_CHECK_ROWS = 10
    
    # åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    check_cancellation(cancel_callback, "XLSXæ–‡ä»¶å¤„ç†")
    
    try:
        excel_data = pd.read_excel(file_path, sheet_name=None, header=None, keep_default_na=False)
    except Exception as e:
        print(f"Error reading Excel file {file_path}: {e}")
        return "", []
    
    # åœ¨è¯»å–Excelæ•°æ®åæ£€æŸ¥å–æ¶ˆçŠ¶æ€
    check_cancellation(cancel_callback, "XLSXæ•°æ®è¯»å–")
    
    for sheet_name, df_initial in excel_data.items():
        # åœ¨å¤„ç†æ¯ä¸ªå·¥ä½œè¡¨å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, f"XLSXå·¥ä½œè¡¨ {sheet_name} å¤„ç†")
            
        if df_initial.empty:
            continue
            
        header_row_index = -1
        best_header_score = -1
        potential_header_idx = -1
        
        # åœ¨è¡¨å¤´æ£€æµ‹å¾ªç¯ä¸­å¢åŠ æ›´é¢‘ç¹çš„å–æ¶ˆæ£€æŸ¥
        for i in range(min(MAX_HEADER_CHECK_ROWS, len(df_initial))):
            # åœ¨æ£€æŸ¥è¡¨å¤´çš„å¾ªç¯ä¸­ä¹Ÿæ·»åŠ å–æ¶ˆæ£€æŸ¥ï¼ˆä½¿ç”¨å‘¨æœŸæ€§æ£€æŸ¥ï¼‰
            periodic_cancellation_check(cancel_callback, 5, i, f"XLSXè¡¨å¤´æ£€æµ‹ {sheet_name}")
                
            row_values = df_initial.iloc[i].tolist()
            current_score = sum(1 for cell in row_values if pd.notna(cell) and isinstance(cell, str) and str(cell).strip() and not str(cell).replace('.', '', 1).isdigit())
            is_plausible = any(isinstance(cell, str) and str(cell).strip() for cell in row_values)
            if is_plausible and current_score > best_header_score:
                best_header_score = current_score
                potential_header_idx = i
            elif best_header_score == -1 and is_plausible:
                potential_header_idx = i
                
        # åœ¨è¡¨å¤´æ£€æµ‹å®Œæˆåæ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, f"XLSXè¡¨å¤´æ£€æµ‹å®Œæˆ {sheet_name}")
            
        if potential_header_idx != -1 and best_header_score >= 1:
            header_row_index = potential_header_idx
            print(f"Detected header in '{sheet_name}' at row {header_row_index + 1}")
            try:
                # åœ¨é‡æ–°è¯»å–å·¥ä½œè¡¨å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                check_cancellation(cancel_callback, f"XLSXé‡æ–°è¯»å–å·¥ä½œè¡¨ {sheet_name}")
                    
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=header_row_index, keep_default_na=False)
                df = df.dropna(axis=0, how='all')
                df = df.dropna(axis=1, how='all')
                df = df.fillna('')
                headers = [str(h).strip() for h in df.columns]
                
                # åœ¨å¼€å§‹å¤„ç†è¡Œæ•°æ®å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                check_cancellation(cancel_callback, f"XLSXå¼€å§‹å¤„ç†è¡Œæ•°æ® {sheet_name}")
                
                row_count = 0
                for idx, row in df.iterrows():
                    # åœ¨å¤„ç†æ¯ä¸€è¡Œæ—¶ä¹Ÿæ£€æŸ¥å–æ¶ˆ - å¢åŠ é¢‘ç‡
                    check_cancellation(cancel_callback, f"XLSXè¡Œå¤„ç† {sheet_name} ç¬¬{idx}è¡Œ")
                    
                    # æ¯å¤„ç†10è¡Œæ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€ï¼ˆæé«˜å“åº”æ€§ï¼‰
                    row_count += 1
                    periodic_cancellation_check(cancel_callback, 10, row_count, f"XLSXæ‰¹é‡è¡Œå¤„ç† {sheet_name}")
                        
                    excel_row_num = idx + header_row_index + 2
                    row_values = [str(cell).strip() for cell in row.tolist()]
                    row_texts_for_full_text = [h for h in headers if h] + [v for v in row_values if v]
                    if row_texts_for_full_text:
                        full_text_list.append(" ".join(row_texts_for_full_text))
                    if any(row_values):
                        searchable_row_text = " ".join(filter(None, row_values))
                        structure.append({
                            'type': 'excel_row',
                            'sheet_name': str(sheet_name),
                            'row_index': excel_row_num,
                            'headers': headers,
                            'values': row_values,
                            'text': searchable_row_text
                        })
                        
            except InterruptedError:
                # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                print(f"XLSXå·¥ä½œè¡¨å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {sheet_name}")
                raise
            except Exception as e:
                print(f"Error re-reading sheet '{sheet_name}' with header at row {header_row_index + 1}: {e}")
                # åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¹Ÿæ£€æŸ¥å–æ¶ˆçŠ¶æ€
                check_cancellation(cancel_callback, f"XLSXå¼‚å¸¸å¤„ç† {sheet_name}")
                sheet_text = df_initial.to_string(index=False, header=False)
                full_text_list.append(sheet_text)
                structure.append({'type': 'paragraph', 'text': f"Content from sheet '{sheet_name}' (header detection failed)."})
        else:
            print(f"Warning: Could not detect header in sheet '{sheet_name}'. Indexing as plain text.")
            # åœ¨å¤„ç†æ— è¡¨å¤´å·¥ä½œè¡¨å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            check_cancellation(cancel_callback, f"XLSXæ— è¡¨å¤´å·¥ä½œè¡¨å¤„ç† {sheet_name}")
                
            sheet_text = df_initial.to_string(index=False, header=False)
            cleaned_sheet_text = "\n".join(line.strip() for line in sheet_text.splitlines() if line.strip())
            if cleaned_sheet_text:
                full_text_list.append(cleaned_sheet_text)
                structure.append({
                    'type': 'paragraph',
                    'text': cleaned_sheet_text,
                    'context': f"Sheet: {sheet_name} (No header detected)"
                })
                
    # åœ¨è¿”å›å‰æœ€åæ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€
    check_cancellation(cancel_callback, "XLSXæ–‡ä»¶å¤„ç†å®Œæˆ")
        
    full_text = '\n'.join(full_text_list)
    return full_text, structure


def extract_text_from_md(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    """Extract text and structure from a Markdown file."""
    # --- ADDED: æ£€æŸ¥Markdownæ”¯æŒè®¸å¯è¯ ---
    if not is_feature_available(Features.MARKDOWN_SUPPORT):
        print(f"Markdownæ”¯æŒåŠŸèƒ½ä¸å¯ç”¨ (æœªè·å¾—è®¸å¯)")
        raise PermissionError("Markdownæ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯ã€‚")
    # ------------------------------------
    
    if not file_path.exists():
        return "", []
    
    content = ""
    structure = []
    encodings_to_try = ['utf-8', 'gbk', 'gb2312']
    try:
        # åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "MDæ–‡ä»¶å¤„ç†å¼€å§‹")
            
        raw_md_content = None
        for encoding in encodings_to_try:
            try:
                raw_md_content = file_path.read_text(encoding=encoding)
                if raw_md_content is not None:
                    break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"Error reading md file {file_path} with encoding {encoding}: {e}")
                return "", []
        if raw_md_content is None:
            print(f"Could not decode md file {file_path} with tried encodings or file is empty.")
            return "", []
        
        # åœ¨è½¬æ¢å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "MDè½¬æ¢HTML")
            
        html_content = markdown.markdown(raw_md_content)
        content = strip_tags(html_content).strip()
        if content:
            structure.append({'type': 'paragraph', 'text': content})
        print(f"MD Extracted Text (first 500 chars): {content[:500]}")
        return content, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except ImportError:
        print("Error: markdown library not installed. Please run: pip install Markdown")
        return "", []

def extract_text_from_html(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    print(f"Attempting to extract text from HTML/HTM: {file_path}")
    content = ""
    structure = []
    try:
        # åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "HTMLæ–‡ä»¶å¤„ç†å¼€å§‹")
            
        raw_html_bytes = file_path.read_bytes()
        if not raw_html_bytes:
            print(f"Warning: HTML/HTM file is empty: {file_path}")
            return "", []
        detected = chardet.detect(raw_html_bytes)
        encoding = detected.get('encoding')
        confidence = detected.get('confidence', 0)
        print(f"Chardet detected encoding: {encoding} with confidence: {confidence:.2f}")
        html_content_decoded = None
        if encoding and confidence > 0.7:
            try:
                html_content_decoded = raw_html_bytes.decode(encoding, errors='replace')
                print(f"Decoded using chardet result: {encoding}")
            except Exception as e:
                print(f"Warning: Failed to decode using detected encoding '{encoding}': {e}")
        if html_content_decoded is None:
            for enc in ['utf-8', 'gbk', 'gb18030', 'latin-1']:
                try:
                    html_content_decoded = raw_html_bytes.decode(enc, errors='strict')
                    print(f"Decoded using fallback encoding: {enc}")
                    break
                except UnicodeDecodeError:
                    continue
            if html_content_decoded is None:
                print(f"Warning: Could not decode HTML/HTM, using utf-8 with replacements.")
                html_content_decoded = raw_html_bytes.decode('utf-8', errors='replace')
        
        # åœ¨è§£æå‰å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "HTMLè§£æå¤„ç†")
            
        soup = BeautifulSoup(html_content_decoded, 'lxml')
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()
        content = ' '.join(soup.stripped_strings)
        if content:
            structure.append({'type': 'paragraph', 'text': content})
        print(f"Successfully extracted HTML/HTM content (length: {len(content)}): {file_path}")
        print(f"Extracted sample: {content[:500]}{'...' if len(content) > 500 else ''}")
        return content, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except Exception as e:
        print(f"Error processing html/htm file {file_path}: {e}")
        # traceback.print_exc(file=sys.stderr) # COMMENTED OUT
        return "", []

def extract_text_from_rtf(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    content = ""
    structure = []
    encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'latin-1']
    try:
        # åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "RTFæ–‡ä»¶å¤„ç†å¼€å§‹")
            
        rtf_content = None
        for encoding in encodings_to_try:
            try:
                rtf_content = file_path.read_text(encoding=encoding)
                if rtf_content is not None and rtf_content.strip().startswith('{\\rtf'):
                    break
                else:
                    rtf_content = None
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"Error reading rtf file {file_path} with encoding {encoding}: {e}")
                return "", []
        if rtf_content is None:
            print(f"Could not decode rtf file {file_path} with tried encodings or file doesn't start with RTF marker.")
            return "", []
        
        # åœ¨è½¬æ¢å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "RTFè½¬æ¢æ–‡æœ¬")
            
        content = rtf_to_text(rtf_content, errors="ignore").strip()
        if content:
            structure.append({'type': 'paragraph', 'text': content})
        return content, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except Exception as e:
        print(f"Error processing rtf file {file_path}: {e}")
        return "", []

def extract_text_from_eml(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    """Extract text from an EML file."""
    # --- ADDED: æ£€æŸ¥é‚®ä»¶æ”¯æŒè®¸å¯è¯ ---
    if not is_feature_available(Features.EMAIL_SUPPORT):
        print(f"é‚®ä»¶æ”¯æŒåŠŸèƒ½ä¸å¯ç”¨ (æœªè·å¾—è®¸å¯)")
        raise PermissionError("é‚®ä»¶æ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯ã€‚")
    # -----------------------------
    
    if not file_path.exists():
        return "", []
    
    full_text_list = []
    structure = []
    try:
        # åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "EMLæ–‡ä»¶å¤„ç†å¼€å§‹")
            
        raw_bytes = file_path.read_bytes()
        if not raw_bytes:
            return "", []
        parser = BytesParser()
        msg = parser.parsebytes(raw_bytes)
        
        # åœ¨è§£æé‚®ä»¶å†…å®¹å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "EMLé‚®ä»¶è§£æ")
            
        def decode_header_simple(header_value):
            if not header_value:
                return ""
            decoded_parts = []
            for part, encoding in decode_header(header_value):
                if isinstance(part, bytes):
                    try:
                        decoded_parts.append(part.decode(encoding or 'utf-8', errors='replace'))
                    except LookupError:
                        decoded_parts.append(part.decode('utf-8', errors='replace'))
                else:
                    decoded_parts.append(part)
            return "".join(decoded_parts)
        subject = decode_header_simple(msg.get('Subject'))
        sender = decode_header_simple(msg.get('From'))
        recipient = decode_header_simple(msg.get('To'))
        cc = decode_header_simple(msg.get('Cc'))
        date_str = msg.get('Date')
        if subject:
            structure.append({'type': 'heading', 'level': 1, 'text': f"ä¸»é¢˜: {subject}"})
            full_text_list.append(subject)
        if sender:
            realname, email_addr = parseaddr(sender)
            clean_sender = f"{realname} <{email_addr}>" if realname else sender
            structure.append({'type': 'metadata', 'text': f"å‘ä»¶äºº: {clean_sender}"})
            full_text_list.append(f"å‘ä»¶äºº: {clean_sender}")
        if recipient:
            structure.append({'type': 'metadata', 'text': f"æ”¶ä»¶äºº: {recipient}"})
            full_text_list.append(f"æ”¶ä»¶äºº: {recipient}")
        if cc:
            structure.append({'type': 'metadata', 'text': f"æŠ„é€: {cc}"})
            full_text_list.append(f"æŠ„é€: {cc}")
        if date_str:
            structure.append({'type': 'metadata', 'text': f"æ—¥æœŸ: {date_str}"})
            full_text_list.append(f"æ—¥æœŸ: {date_str}")
        body_content = ""
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                content_disposition = str(part.get('Content-Disposition'))
                if 'attachment' not in content_disposition and content_type.startswith('text/'):
                    payload = part.get_payload(decode=True)
                    charset = part.get_content_charset() or 'utf-8'
                    try:
                        body_content += payload.decode(charset, errors='replace')
                    except LookupError:
                        body_content += payload.decode('utf-8', errors='replace')
        else:
            payload = msg.get_payload(decode=True)
            charset = msg.get_content_charset() or 'utf-8'
            try:
                body_content = payload.decode(charset, errors='replace')
            except LookupError:
                body_content = payload.decode('utf-8', errors='replace')
        if body_content:
            lines = body_content.splitlines()
            for line in lines:
                cleaned_line = line.strip()
                if cleaned_line:
                    structure.append({'type': 'paragraph', 'text': cleaned_line})
                    full_text_list.append(cleaned_line)
        full_text = '\n'.join(full_text_list)
        return full_text, structure
    except Exception as e:
        print(f"Error extracting EML {file_path}: {e}")
        # traceback.print_exc(file=sys.stderr) # COMMENTED OUT
        return "", []

def extract_text_from_msg(file_path: Path, cancel_callback=None) -> tuple[str, list[dict]]:
    # --- ADDED: æ£€æŸ¥é‚®ä»¶æ”¯æŒè®¸å¯è¯ ---
    if not is_feature_available(Features.EMAIL_SUPPORT):
        print(f"é‚®ä»¶æ”¯æŒåŠŸèƒ½ä¸å¯ç”¨ (æœªè·å¾—è®¸å¯)")
        raise PermissionError("é‚®ä»¶æ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯ã€‚")
    # -----------------------------
    
    print(f"Attempting to extract text from MSG: {file_path.name}")
    full_text_list = []
    structure = []
    try:
        msg = extract_msg.Message(str(file_path))
        if msg.subject:
            structure.append({'type': 'heading', 'level': 1, 'text': f"ä¸»é¢˜: {msg.subject}"})
            full_text_list.append(msg.subject)
        if msg.sender:
            structure.append({'type': 'metadata', 'text': f"å‘ä»¶äºº: {msg.sender}"})
            full_text_list.append(f"å‘ä»¶äºº: {msg.sender}")
        if msg.to:
            structure.append({'type': 'metadata', 'text': f"æ”¶ä»¶äºº: {msg.to}"})
            full_text_list.append(f"æ”¶ä»¶äºº: {msg.to}")
        if msg.cc:
            structure.append({'type': 'metadata', 'text': f"æŠ„é€: {msg.cc}"})
            full_text_list.append(f"æŠ„é€: {msg.cc}")
        if msg.date:
            structure.append({'type': 'metadata', 'text': f"æ—¥æœŸ: {msg.date}"})
            full_text_list.append(f"æ—¥æœŸ: {msg.date}")
        body_to_process = None
        processed_html_body = False
        
        # åœ¨å¤„ç†é‚®ä»¶æ­£æ–‡å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        check_cancellation(cancel_callback, "MSGé‚®ä»¶æ­£æ–‡å¤„ç†")
            
        if hasattr(msg, 'htmlBody') and msg.htmlBody:
            print("MSG Body: Attempting to process HTML body.")
            html_bytes = None
            try:
                if isinstance(msg.htmlBody, bytes):
                    html_bytes = msg.htmlBody
                elif isinstance(msg.htmlBody, str):
                    html_bytes = msg.htmlBody.encode('utf-8', errors='ignore')
                if html_bytes:
                    detected = chardet.detect(html_bytes)
                    encoding = detected.get('encoding')
                    confidence = detected.get('confidence', 0)
                    html_content_decoded = None
                    if encoding and confidence > 0.7:
                        try:
                            html_content_decoded = html_bytes.decode(encoding, errors='replace')
                            print(f"MSG HTML Body: Decoded using chardet result: {encoding}")
                        except Exception as e:
                            print(f"MSG HTML Body: Warning - Failed using chardet '{encoding}': {e}")
                    if html_content_decoded is None:
                        for enc in ['utf-8', 'gbk', 'gb18030']:
                            try:
                                html_content_decoded = html_bytes.decode(enc, errors='strict')
                                print(f"MSG HTML Body: Decoded using fallback: {enc}")
                                break
                            except Exception:
                                pass  # ç»§ç»­å°è¯•å…¶ä»–ç¼–ç 
                        if html_content_decoded is None:
                            html_content_decoded = html_bytes.decode('utf-8', errors='replace')
                            print(f"MSG HTML Body: Decoded using final fallback utf-8 (replace)")
                    if html_content_decoded:
                        soup = BeautifulSoup(html_content_decoded, 'lxml')
                        for script_or_style in soup(["script", "style"]):
                            script_or_style.decompose()
                        body_to_process = ' '.join(soup.stripped_strings)
                        if body_to_process:
                            print(f"MSG Body: Successfully processed HTML body (length {len(body_to_process)}).")
                            processed_html_body = True
                        else:
                            print("MSG Body: Processed HTML body resulted in empty text.")
                else:
                    print("MSG Body: msg.htmlBody was string but failed to re-encode to bytes.")
            except Exception as e:
                print(f"MSG Body: Error processing HTML body: {e}")
                body_to_process = None
        if not processed_html_body:
            print("MSG Body: HTML body not found or failed, falling back to plain text body.")
            if msg.body:
                body_text = msg.body
                print(f"MSG Body (Raw Plain Text Fallback, first 500 chars): {body_text[:500]}")
                body_to_process = body_text
            else:
                print("MSG Body: Plain text body is also empty.")
                body_to_process = ""
        if body_to_process:
            body_lines = body_to_process.splitlines()
            for line in body_lines:
                cleaned_line = line.strip()
                if cleaned_line:
                    structure.append({'type': 'paragraph', 'text': cleaned_line})
                    full_text_list.append(cleaned_line)
        else:
            print("MSG Body: No processable body content found (HTML or Plain Text).")
        full_text = '\n'.join(full_text_list)
        print(f"Extracted full text from {file_path.name} (first 500 chars): {full_text[:500]}")
        return full_text, structure
    except InterruptedError:
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        raise
    except Exception as e:
        print(f"Error processing MSG file {file_path}: {e}")
        # traceback.print_exc(file=sys.stderr) # COMMENTED OUT
        return "", []

def index_documents(writer, content_dict: dict[Path, tuple[str, list[dict]]]):
    """å°†æå–çš„å†…å®¹æ·»åŠ åˆ°ç´¢å¼•ä¸­
    
    Args:
        writer: Whoosh writer
        content_dict: è·¯å¾„åˆ°å†…å®¹çš„æ˜ å°„
    """
    for path, (content, structure_map) in content_dict.items():
        try:
            # è·å–æ–‡ä»¶ç±»å‹
            if isinstance(path, str):
                file_path = path.split('::')[0] if '::' in path else path  # å¤„ç†å­˜æ¡£è·¯å¾„
                file_ext = Path(file_path.split('::')[-1] if '::' in file_path else file_path).suffix.lower()
            else:
                file_path = str(path)
                file_ext = path.suffix.lower()
            
            # è·å–æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
            file_size = content_dict.get(path, {}).get('file_size', 0)
            last_modified = content_dict.get(path, {}).get('last_modified', 0)
            
            # è·å–OCRçŠ¶æ€
            ocr_used = content_dict.get(path, {}).get('ocr_used', False)
            
            # æå–æ–‡ä»¶åï¼ˆç”¨äºæ–‡ä»¶åæœç´¢ï¼‰
            if isinstance(path, str):
                norm_path = normalize_path_for_index(path)
                if '::' in norm_path:
                    # å¤„ç†å­˜æ¡£å†…æ–‡ä»¶
                    _, member_path = norm_path.split('::', 1)
                    filename_text = Path(member_path).name
                else:
                    # æ™®é€šæ–‡ä»¶
                    filename_text = Path(norm_path).name
            else:
                # Pathå¯¹è±¡
                filename_text = path.name

            # æ›´æ–°ç´¢å¼•
            writer.update_document(
                path=normalize_path_for_index(str(path)),  # æ ‡å‡†åŒ–è·¯å¾„
                content=content,
                filename_text=filename_text,  # æ–‡ä»¶åä½œä¸ºå•ç‹¬çš„å­—æ®µ
                structure_map=structure_map,
                last_modified=last_modified,
                file_size=file_size,
                file_type=file_ext.lstrip('.'),  # å»æ‰å‰å¯¼ç‚¹
                indexed_with_ocr=ocr_used  # å­˜å‚¨OCRä½¿ç”¨çŠ¶æ€
            )
        except Exception as e:
            print(f"Warning: Error indexing document {path}: {e}")
            continue  # ç»§ç»­ç´¢å¼•å…¶ä»–æ–‡æ¡£

def get_positive_terms(q: Query) -> set[str]:
    """Recursively extract positive terms (not under a NOT) from a query tree."""
    positive_terms = set()
    if isinstance(q, Term):
        positive_terms.add(q.text)
    elif isinstance(q, (And, Or)):
        for subq in q.children():
            positive_terms.update(get_positive_terms(subq))
    elif isinstance(q, Phrase):
        positive_terms.update(q.words)
    # --- ADDED: å¤„ç†WildcardæŸ¥è¯¢ ---
    elif isinstance(q, Wildcard):
        # å»é™¤é€šé…ç¬¦ï¼Œæå–å…³é”®éƒ¨åˆ†è¿›è¡Œé«˜äº®
        # ä¾‹å¦‚ "é¡¹ç›®*è®¡åˆ’" ä¼šæå– "é¡¹ç›®" å’Œ "è®¡åˆ’" è¿›è¡Œé«˜äº®
        term = q.text
        # åˆ†å‰²é€šé…ç¬¦
        parts = []
        
        # å¦‚æœä»¥*å¼€å¤´ï¼Œå»æ‰å¼€å¤´çš„*
        if term.startswith('*'):
            term = term[1:]
            
        # å¦‚æœä»¥*ç»“å°¾ï¼Œå»æ‰ç»“å°¾çš„*
        if term.endswith('*'):
            term = term[:-1]
            
        # ä½¿ç”¨é€šé…ç¬¦*åˆ†å‰²å­—ç¬¦ä¸²
        if '*' in term:
            parts = [p for p in term.split('*') if p]
        elif '?' in term:
            # å¯¹äº?é€šé…ç¬¦ï¼Œç®€å•åœ°å»é™¤
            parts = [term.replace('?', '')]
        else:
            parts = [term]
            
        # æ·»åŠ æ‰€æœ‰åˆ†å‰²åçš„éƒ¨åˆ†åˆ°é«˜äº®è¯é›†åˆ
        for part in parts:
            if part:  # ç¡®ä¿ä¸æ·»åŠ ç©ºå­—ç¬¦ä¸²
                positive_terms.add(part)
    # --- END ADDED ---
    # Ignore terms under Not, Prefix, Wildcard, Every, etc. for highlighting purposes
    # You might want to refine this for Prefix/Wildcard if needed
    return positive_terms

# --- ADDED BACK: convert_term_to_prefix function --- 
def convert_term_to_prefix(q: Query, fieldname: str = "content") -> Query:
    """Recursively convert Term queries to Prefix queries within a Query tree."""
    if isinstance(q, Term) and q.fieldname == fieldname:
        # Only convert Terms in the specified field
        return Prefix(q.fieldname, q.text)
    elif isinstance(q, (And, Or)):
        # Recursively process children of And/Or groups
        return q.__class__([convert_term_to_prefix(subq, fieldname) for subq in q.children()])
    elif isinstance(q, Not):
        # For Not queries, process the *inner* query
        # Whoosh Not object holds its child query in the .query attribute
        return Not(convert_term_to_prefix(q.query, fieldname))
    else:
        # Return other query types (like Prefix, Wildcard, Every, etc.) unchanged
        return q
# --- END ADDED BACK --- 

# --- ADDED: éªŒè¯é€šé…ç¬¦è¯­æ³•çš„å‡½æ•° ---
def validate_wildcard_syntax(query_str):
    """
    éªŒè¯é€šé…ç¬¦è¯­æ³•æ˜¯å¦åˆæ³•
    è¿”å›: (bool, str) - (æ˜¯å¦åˆæ³•, é”™è¯¯ä¿¡æ¯)
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„è½¬ä¹‰ç¬¦
    if query_str.endswith('\\'):
        return False, "é€šé…ç¬¦æŸ¥è¯¢ä¸èƒ½ä»¥è½¬ä¹‰å­—ç¬¦\\ç»“å°¾"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­å¤šä¸ª*å·ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜
    if '**' in query_str:
        return False, "è¿ç»­çš„**é€šé…ç¬¦å¯èƒ½å¯¼è‡´æœç´¢ç¼“æ…¢ï¼Œè¯·ä½¿ç”¨å•ä¸ª*"
        
    # æ£€æŸ¥æ˜¯å¦åªæœ‰é€šé…ç¬¦
    if query_str.strip() in ['*', '?', '*?', '?*']:
        return False, "æŸ¥è¯¢ä¸èƒ½ä»…åŒ…å«é€šé…ç¬¦ï¼Œè¯·æ·»åŠ è‡³å°‘ä¸€ä¸ªå­—ç¬¦"
        
    return True, ""

# --- ADDED: æ£€æŸ¥é€šé…ç¬¦æŸ¥è¯¢æ€§èƒ½é£é™© ---
def check_wildcard_performance_risk(query_str):
    """
    æ£€æŸ¥é€šé…ç¬¦æŸ¥è¯¢æ˜¯å¦å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜
    è¿”å›: (bool, str) - (æ˜¯å¦æœ‰é£é™©, é£é™©æè¿°)
    """
    # æ£€æŸ¥æ˜¯å¦ä»¥*å¼€å¤´
    if query_str.startswith('*'):
        return True, "ä»¥*å¼€å¤´çš„æŸ¥è¯¢å¯èƒ½è¾ƒæ…¢ï¼Œå› ä¸ºæ— æ³•ä½¿ç”¨ç´¢å¼•å‰ç¼€ä¼˜åŒ–"
    
    # æ£€æŸ¥é€šé…ç¬¦æ•°é‡æ˜¯å¦è¿‡å¤š
    wildcard_count = query_str.count('*') + query_str.count('?')
    if wildcard_count > 3:
        return True, f"æŸ¥è¯¢åŒ…å«{wildcard_count}ä¸ªé€šé…ç¬¦ï¼Œå¯èƒ½å¯¼è‡´æœç´¢è¾ƒæ…¢"
        
    return False, ""

# --- ADDED: ç»Ÿä¸€å¤„ç†é€šé…ç¬¦æŸ¥è¯¢ ---
def process_wildcard_query(query_str, is_filename_search=False):
    """
    ç»Ÿä¸€å¤„ç†é€šé…ç¬¦æŸ¥è¯¢
    is_filename_search: æ˜¯å¦ä¸ºæ–‡ä»¶åæœç´¢
    è¿”å›: å¤„ç†åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    has_wildcard = '*' in query_str or '?' in query_str
    
    # æ–‡ä»¶åæœç´¢é»˜è®¤æ·»åŠ é€šé…ç¬¦
    if is_filename_search and not has_wildcard:
        return f"*{query_str}*"
    
    # ä¸­æ–‡é€šé…ç¬¦æœç´¢å¢å¼ºï¼šå¯¹äºä¸­æ–‡å’Œæ•°å­—æ··åˆçš„æƒ…å†µï¼Œæé«˜åŒ¹é…çµæ´»åº¦
    if has_wildcard and not is_filename_search:
        # ç¡®ä¿*å‰åçš„è¯èƒ½å¤Ÿæ›´å¥½åœ°åˆ†å‰²å’ŒåŒ¹é…
        # ä¾‹å¦‚ï¼šå°†"åä¹å±Š*å…¨ä¼š"è½¬æ¢ä¸ºæ›´çµæ´»çš„åŒ¹é…æ¨¡å¼
        if '*' in query_str:
            parts = query_str.split('*')
            for i in range(len(parts) - 1):
                if parts[i] and parts[i+1]:  # ç¡®ä¿ä¸æ˜¯å¼€å¤´æˆ–ç»“å°¾çš„*
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å’Œæ•°å­—æ··åˆ
                    has_chinese = any('\u4e00' <= c <= '\u9fff' for c in parts[i]+parts[i+1])
                    if has_chinese:
                        # ä¸ºæé«˜çµæ´»æ€§ï¼Œå¯¹è¿æ¥å¤„è¿›è¡Œç‰¹æ®Šå¤„ç†
                        print(f"å¢å¼ºä¸­æ–‡é€šé…ç¬¦æœç´¢: '{query_str}'")
            # å¯¹äºä¸­æ–‡é€šé…ç¬¦æœç´¢ï¼Œæš‚æ—¶ä¿æŒåŸæ ·ï¼Œä¸»è¦é€šè¿‡Whooshçš„WildcardæŸ¥è¯¢å®ç°
        
    # å…¨æ–‡æœç´¢ä¿æŒåŸæ ·
    return query_str

# å¢åŠ ä¸€ä¸ªè¾…åŠ©å‡½æ•°ç”¨äºæ‰©å±•ä¸­æ–‡é€šé…ç¬¦æœç´¢
def expand_chinese_wildcard_query(query_str):
    """
    æ‰©å±•ä¸­æ–‡é€šé…ç¬¦æŸ¥è¯¢ï¼Œå¢åŠ æ›´å¤šå¯èƒ½çš„åŒ¹é…æ¨¡å¼
    ä¾‹å¦‚ï¼šåä¹å±Š*å…¨ä¼š â†’ åä¹å±Š*å…¨ä¼š OR åä¹*å…¨ä¼š
    """
    if '*' not in query_str and '?' not in query_str:
        return query_str
        
    expanded_queries = [query_str]
    
    # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—+ä¸­æ–‡çš„æ¨¡å¼ï¼Œå¦‚"åä¹å±Š*å…¨ä¼š"
    parts = re.split(r'([*?])', query_str)
    pattern = re.compile(r'[\u4e00-\u9fff]+[\d]+[\u4e00-\u9fff]+|[\d]+[\u4e00-\u9fff]+|[\u4e00-\u9fff]+[\d]+')
    
    for i in range(0, len(parts), 2):
        if i < len(parts) and pattern.search(parts[i]):
            # åŒ…å«ä¸­æ–‡å’Œæ•°å­—æ··åˆï¼Œè€ƒè™‘æ›´çµæ´»çš„åŒ¹é…
            if 'å±Š' in parts[i]:
                # ç‰¹æ®Šå¤„ç†åŒ…å«"å±Š"çš„æƒ…å†µï¼Œå¦‚"åä¹å±Š"
                if i+2 < len(parts):
                    new_query = parts[i].replace('å±Š', '') + parts[i+1] + parts[i+2]
                    expanded_queries.append(new_query)
                    print(f"æ‰©å±•é€šé…ç¬¦æŸ¥è¯¢: æ·»åŠ  '{new_query}'")
    
    # æ·»åŠ æ›´å¤šçš„ä¸­æ–‡é€šé…ç¬¦ç‰¹æ®Šå¤„ç†è§„åˆ™
    # å¤„ç†å¸¸è§çš„ä¸­æ–‡è¯ç»„åˆ†è¯é—®é¢˜
    if '*' in query_str:
        parts = query_str.split('*')
        # ç‰¹æ®Šå¤„ç†ä¸­æ–‡è¯ç»„é—´çš„é€šé…ç¬¦
        for i in range(len(parts) - 1):
            if parts[i] and parts[i+1] and all('\u4e00' <= c <= '\u9fff' for c in parts[i][-1] + parts[i+1][0]):
                # å°è¯•ä¸åŒçš„è¯ç»„åˆ‡åˆ†æ–¹å¼
                if len(parts[i]) > 1:
                    # æ‹†åˆ†å‰ä¸€ä¸ªè¯çš„æœ€åä¸€ä¸ªå­—ç¬¦
                    new_query = parts[i][:-1] + '*' + parts[i][-1] + parts[i+1]
                    if new_query != query_str and new_query not in expanded_queries:
                        expanded_queries.append(new_query)
                        print(f"ä¸­æ–‡åˆ†è¯ä¼˜åŒ–: æ·»åŠ  '{new_query}'")
                
                if len(parts[i+1]) > 1:
                    # æ‹†åˆ†åä¸€ä¸ªè¯çš„ç¬¬ä¸€ä¸ªå­—ç¬¦
                    new_query = parts[i] + parts[i+1][0] + '*' + parts[i+1][1:]
                    if new_query != query_str and new_query not in expanded_queries:
                        expanded_queries.append(new_query)
                        print(f"ä¸­æ–‡åˆ†è¯ä¼˜åŒ–: æ·»åŠ  '{new_query}'")
    
    if len(expanded_queries) > 1:
        # ä½¿ç”¨ORç»„åˆæŸ¥è¯¢
        print(f"æ‰©å±•åçš„æŸ¥è¯¢é€‰é¡¹: {expanded_queries}")
    
    return expanded_queries  # è¿”å›æ‰€æœ‰æ‰©å±•æŸ¥è¯¢ï¼Œä¸Šå±‚è°ƒç”¨å¤„éœ€è¦å¤„ç†

def search_index(query_str: str,
                 index_dir_path: str, # Added parameter
                 search_mode: str = 'phrase',
                 # --- ADDED: Parameter for search scope ---
                 search_scope: str = 'fulltext',
                 # -----------------------------------------
                 min_size_kb: int | None = None,
                 max_size_kb: int | None = None,
                 start_date: str | None = None,  # Changed to string (e.g., "2023-01-01")
                 end_date: str | None = None,    # Changed to string (e.g., "2023-12-31")
                 file_type_filter: list[str] | None = None,
                 sort_by: str = 'relevance',
                 case_sensitive: bool = False,
                 # --- ADDED: Parameter for current source directories ---
                 current_source_dirs: list[str] | None = None) -> list[dict]: # Filter results by current directories
    # --- MODIFIED: Include search_scope in debug log ---
    print(f"\nğŸ” å¼€å§‹æœç´¢:")
    print(f"   æŸ¥è¯¢: '{query_str}'")
    print(f"   æ¨¡å¼: {search_mode}")
    print(f"   èŒƒå›´: {search_scope}")
    print(f"   ç´¢å¼•: {index_dir_path}")
    if search_mode == 'phrase':
        print(f"   âš ï¸  ç²¾ç¡®æœç´¢æ¨¡å¼: å°†åªè¿”å›åŒ…å«å®Œæ•´çŸ­è¯­ '{query_str}' çš„ç»“æœ")
    # ---------------------------------------------------
    print(f"Filters - Size: {min_size_kb}-{max_size_kb}KB, Date: {start_date}-{end_date}, Types: {file_type_filter}") # Debug
    print(f"Case Sensitive: {case_sensitive} (Note: Currently ignored by backend)") # ADDED Debug for case_sensitive

    processed_results = [] # <--- Initialize a new list to store processed hits
    if not Path(index_dir_path).exists() or not exists_in(index_dir_path):
        print(f"Error: Index directory '{index_dir_path}' not found.")
        return processed_results # <--- Return the empty processed list

    # --- æ£€æŸ¥é€šé…ç¬¦æœç´¢æ˜¯å¦å…è®¸ (ä½ç½®1ï¼šåœ¨æœ€å¼€å§‹æ£€æŸ¥) ---
    if '*' in query_str or '?' in query_str:
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸“ä¸šç‰ˆåŠŸèƒ½
        if not is_feature_available(Features.WILDCARDS):
            print(f"é€šé…ç¬¦æœç´¢åŠŸèƒ½ä¸å¯ç”¨ (æœªè·å¾—è®¸å¯)")
            return [{'error': True, 'error_message': "é€šé…ç¬¦æœç´¢æ˜¯ä¸“ä¸šç‰ˆåŠŸèƒ½ï¼Œè¯·å‡çº§ä»¥ä½¿ç”¨", 'license_required': True}]
        
        # éªŒè¯é€šé…ç¬¦è¯­æ³•
        is_valid, error_message = validate_wildcard_syntax(query_str)
        if not is_valid:
            print(f"æ— æ•ˆçš„é€šé…ç¬¦è¯­æ³•: {error_message}")
            return [{'error': True, 'error_message': f"é€šé…ç¬¦è¯­æ³•é”™è¯¯: {error_message}"}]
        
        # æ£€æŸ¥æ€§èƒ½é£é™©
        has_risk, risk_message = check_wildcard_performance_risk(query_str)
        if has_risk:
            print(f"é€šé…ç¬¦æ€§èƒ½é£é™©: {risk_message}")
            # ä»…è®°å½•é£é™©ï¼Œä¸é˜»æ­¢æœç´¢ï¼Œå¯é€‰æ‹©æ·»åŠ åˆ°ç»“æœä¸­ä½œä¸ºè­¦å‘Š
            processed_results.append({'warning': True, 'warning_message': risk_message, 'performance_warning': True})

    # --- Determine target field based on scope --- ADDED
    target_field = "filename_text" if search_scope == 'filename' else "content"
    print(f"Searching in field: '{target_field}'")
    # --------------------------------------------
    
    # --- æ£€æŸ¥è®¸å¯è¯çŠ¶æ€ï¼Œç¡®å®šå½“å‰å¯è®¿é—®çš„æ–‡ä»¶ç±»å‹ ---
    allowed_file_types = {'.docx', '.txt', '.html', '.htm', '.rtf', '.xlsx', '.pptx'} # åŸºç¡€ç‰ˆå…è®¸çš„æ–‡ä»¶ç±»å‹
    
    # å¦‚æœç”¨æˆ·æœ‰ç›¸åº”çš„è®¸å¯è¯ï¼Œæ·»åŠ ä¸“ä¸šç‰ˆæ–‡ä»¶ç±»å‹
    if is_feature_available(Features.PDF_SUPPORT):
        allowed_file_types.add('.pdf')
    if is_feature_available(Features.MARKDOWN_SUPPORT):
        allowed_file_types.add('.md')
    if is_feature_available(Features.EMAIL_SUPPORT):
        allowed_file_types.add('.eml')
        allowed_file_types.add('.msg')
    if is_feature_available(Features.MULTIMEDIA_SUPPORT):
        # æ·»åŠ å¤šåª’ä½“æ–‡ä»¶ç±»å‹æ”¯æŒ
        # è§†é¢‘æ–‡ä»¶
        allowed_file_types.update(['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.3gp', '.rmvb'])
        # éŸ³é¢‘æ–‡ä»¶  
        allowed_file_types.update(['.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a', '.opus', '.aiff'])
        # å›¾ç‰‡æ–‡ä»¶
        allowed_file_types.update(['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.tiff', '.svg', '.ico', '.raw'])
    
    print(f"Current license allows file types: {allowed_file_types}")
    # -------------------------------------------

    ix = open_dir(index_dir_path)
    searcher = ix.searcher(weighting=scoring.BM25F())
    # --- Use analyzer associated with the target field (or default) --- MODIFIED
    analyzer = ix.schema[target_field].analyzer if target_field in ix.schema else ChineseAnalyzer()
    # -------------------------------------------------------------------

    text_query = None
    parsed_query_obj = None # Store the parsed query object
    if query_str:
        # --- å¤„ç†ä¸­æ–‡é€šé…ç¬¦ç‰¹æ®Šæƒ…å†µ ---
        has_wildcard = '*' in query_str or '?' in query_str
        chinese_wildcard_expansion = False
        expanded_queries = []
        
        if has_wildcard:
            # æ£€æµ‹æ˜¯å¦æ˜¯éœ€è¦æ‰©å±•çš„ä¸­æ–‡é€šé…ç¬¦ç‰¹æ®Šæƒ…å†µ
            if any('\u4e00' <= c <= '\u9fff' for c in query_str):  # åŒ…å«ä¸­æ–‡å­—ç¬¦
                # ç‰¹åˆ«å¤„ç†ç±»ä¼¼"åä¹å±Š*å…¨ä¼š"è¿™æ ·çš„æŸ¥è¯¢
                if 'å±Š*' in query_str or 'æ¬¡*' in query_str or '*å…¨ä¼š' in query_str:
                    chinese_wildcard_expansion = True
                    expanded_queries = []
                    
                    # åŸå§‹æŸ¥è¯¢
                    expanded_queries.append(query_str)
                    
                    # æ‰©å±•æŸ¥è¯¢1ï¼šå¤„ç†"å±Š"
                    if 'å±Š*' in query_str:
                        expanded_queries.append(query_str.replace('å±Š*', '*'))
                        # å¤„ç†ç±»ä¼¼"åä¹å±Š*å…¨ä¼š"ä¸"åä¹å±Šå†æ¬¡å…¨ä¼š"çš„åŒ¹é…
                        if 'å…¨ä¼š' in query_str:
                            expanded_queries.append(query_str.replace('å±Š*', 'å±Šå†æ¬¡'))
                            expanded_queries.append(query_str.replace('*å…¨ä¼š', 'å†æ¬¡å…¨ä¼š'))
                    
                    # æ‰©å±•æŸ¥è¯¢2ï¼šå¢åŠ æ›´å¤šå¯èƒ½çš„åŒ¹é…
                    if '*å…¨ä¼š' in query_str:
                        prefix_part = query_str.split('*')[0]
                        expanded_queries.append(f"{prefix_part}*ä¼šè®®")
                        
                        # ç‰¹åˆ«å¤„ç†åŒ…å«"å±Š"çš„å‰ç¼€
                        if 'å±Š' in prefix_part:
                            base_prefix = prefix_part.split('å±Š')[0] + 'å±Š'
                            expanded_queries.append(f"{base_prefix}å†æ¬¡*")
                            expanded_queries.append(f"{base_prefix}å†æ¬¡å…¨ä¼š")
                        
                        # ç‰¹åˆ«å¤„ç†"åä¹å±Š"
                        if 'åä¹å±Š' in prefix_part:
                            expanded_queries.append(prefix_part.replace('åä¹å±Š', 'åä¹*'))
                            expanded_queries.append('åä¹*å…¨ä¼š')
                            expanded_queries.append('åä¹å±Šå†æ¬¡å…¨ä¼š')

                # é€šç”¨ä¸­æ–‡é€šé…ç¬¦ä¼˜åŒ–æ‰©å±•
                if not expanded_queries:
                    # å¦‚æœæ²¡æœ‰é€šè¿‡ç‰¹å®šè§„åˆ™æ‰©å±•ï¼Œä½¿ç”¨é€šç”¨æ‰©å±•
                    general_expanded = expand_chinese_wildcard_query(query_str)
                    if isinstance(general_expanded, list) and len(general_expanded) > 1:
                        chinese_wildcard_expansion = True
                        expanded_queries = general_expanded
                    
                print(f"ä¸­æ–‡é€šé…ç¬¦æœç´¢æ‰©å±•: åŸå§‹æŸ¥è¯¢ '{query_str}' æ‰©å±•ä¸º {expanded_queries}")
        
        # --- MODIFIED: ä½¿ç”¨ç»Ÿä¸€çš„é€šé…ç¬¦å¤„ç†å‡½æ•° --- 
        if search_scope == 'filename':
            target_field = "filename_text"
            analyzer = ix.schema[target_field].analyzer if target_field in ix.schema else analysis.StandardAnalyzer() # Ensure standard analyzer for filename
            
            # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°å¤„ç†æ–‡ä»¶åæœç´¢çš„é€šé…ç¬¦
            processed_query = process_wildcard_query(query_str, is_filename_search=True)
            text_query = Wildcard(target_field, processed_query)
            print(f"Constructed Wildcard query for filename: {text_query}")
        else: # Handle fulltext search based on search_mode
            target_field = "content"
            # --- FIXED: å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬ä¿®æ”¹åçš„ChineseAnalyzerï¼Œæ”¯æŒphrase_mode ---
            analyzer = ChineseAnalyzer()
            # --- ä¿®æ”¹å…¨æ–‡æ¨¡ç³Šæœç´¢ä¸­çš„é€šé…ç¬¦å¤„ç†é€»è¾‘ --- 
            if search_mode == 'phrase':
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é€»è¾‘æ“ä½œç¬¦
                logical_operators = ['AND', 'OR', 'NOT']
                has_logical_operators = any(f" {op} " in f" {query_str} " for op in logical_operators)
                
                # --- ADDED: æ£€æµ‹é€šé…ç¬¦ï¼Œåœ¨ç²¾ç¡®æ¨¡å¼ä¸‹ä¹Ÿæ”¯æŒ --- 
                has_wildcard = '*' in query_str or '?' in query_str
                
                if has_wildcard:
                    # --- å¤„ç†ä¸­æ–‡é€šé…ç¬¦ç‰¹æ®Šæ‰©å±•æŸ¥è¯¢ ---
                    if chinese_wildcard_expansion and expanded_queries:
                        # ä½¿ç”¨ORç»„åˆå¤šä¸ªæŸ¥è¯¢
                        sub_queries = []
                        for exp_query in expanded_queries:
                            sub_queries.append(Wildcard(target_field, exp_query))
                            print(f"Added wildcard expansion: {exp_query}")
                            
                        if len(sub_queries) == 1:
                            text_query = sub_queries[0]
                        else:
                            text_query = Or(sub_queries)
                            print(f"Created combined OR query with {len(sub_queries)} expansions")
                    else:
                        # åœ¨ç²¾ç¡®æ¨¡å¼ä¸‹ä¹Ÿæ”¯æŒé€šé…ç¬¦
                        print(f"Wildcard detected in phrase mode for '{target_field}': '{query_str}'. Using wildcard query.")
                        processed_query = process_wildcard_query(query_str, is_filename_search=False)
                        text_query = Wildcard(target_field, processed_query)
                        parsed_query_obj = text_query
                        print(f"Constructed Wildcard query in phrase mode on '{target_field}': {text_query}")
                elif has_logical_operators:
                    # åœ¨ç²¾ç¡®æœç´¢æ¨¡å¼ä¸‹ä¸å¤„ç†é€»è¾‘æ“ä½œç¬¦ï¼Œç›´æ¥ä½¿ç”¨çŸ­è¯­æœç´¢
                    print(f"WARNING: Logical operators detected in phrase mode for query: '{query_str}'. These operators are only supported in fuzzy mode.")
                    # --- MODIFIED: ä¸ºç²¾ç¡®æœç´¢ä¼ é€’phrase_modeå‚æ•° ---
                    terms = [token.text for token in analyzer(query_str, phrase_mode=True)]
                    if terms:
                        text_query = Phrase(target_field, terms)
                        print(f"Constructed Phrase query on '{target_field}': {text_query}")
                        if text_query:
                            parsed_query_obj = text_query # Store phrase query object
                    else:
                        print(f"Phrase query for '{target_field}' is empty after analysis.")
                else:
                    # é‡æ–°è®¾è®¡çš„çœŸæ­£ç²¾ç¡®æœç´¢ï¼šå­—ç¬¦ä¸²åŒ…å«åŒ¹é…
                    # --- REDESIGNED: çœŸæ­£çš„ç²¾ç¡®æœç´¢å®ç° ---
                    
                    # ç²¾ç¡®æœç´¢çš„æ ¸å¿ƒæ€æƒ³ï¼šæŸ¥æ‰¾åŒ…å«å®Œå…¨ç›¸åŒå­—ç¬¦ä¸²çš„æ–‡æ¡£
                    # ä¸ä¾èµ–åˆ†è¯ï¼Œç›´æ¥è¿›è¡Œå­—ç¬¦ä¸²åŒ¹é…
                    
                    print(f"ç²¾ç¡®æœç´¢é‡æ–°è®¾è®¡ï¼šæŸ¥æ‰¾åŒ…å« '{query_str}' çš„æ–‡æ¡£")
                    
                    # ç­–ç•¥1: å°è¯•å®Œæ•´å­—ç¬¦ä¸²çš„TermåŒ¹é…ï¼ˆé€‚ç”¨äºç´¢å¼•ä¸­æ°å¥½æœ‰è¯¥è¯æ±‡çš„æƒ…å†µï¼‰
                    strategies = []
                    
                    # é¦–å…ˆå°è¯•å°†æ•´ä¸ªæŸ¥è¯¢ä½œä¸ºå•ä¸ªTerm
                    strategies.append(Term(target_field, query_str))
                    print(f"ç­–ç•¥1: å®Œæ•´TermåŒ¹é… - {query_str}")
                    
                    # ç­–ç•¥2: å‰ç¼€åŒ¹é…ï¼ˆé€‚ç”¨äºæŸ¥è¯¢æ˜¯æŸä¸ªæ›´é•¿è¯æ±‡å‰ç¼€çš„æƒ…å†µï¼‰
                    if len(query_str) >= 2:
                        strategies.append(Prefix(target_field, query_str))
                        print(f"ç­–ç•¥2: å‰ç¼€åŒ¹é… - {query_str}*")
                    
                    # ç­–ç•¥3: é€šé…ç¬¦åŒ¹é…ï¼ˆåœ¨æŸ¥è¯¢å‰ååŠ é€šé…ç¬¦ï¼Œå¯»æ‰¾åŒ…å«è¯¥å­—ç¬¦ä¸²çš„è¯æ±‡ï¼‰
                    if len(query_str) >= 2:
                        wildcard_pattern = f"*{query_str}*"
                        strategies.append(Wildcard(target_field, wildcard_pattern))
                        print(f"ç­–ç•¥3: é€šé…ç¬¦åŒ…å«åŒ¹é… - {wildcard_pattern}")
                    
                    # ç­–ç•¥4: å¦‚æœå‰é¢éƒ½å¤±è´¥ï¼Œå›é€€åˆ°åˆ†è¯çŸ­è¯­åŒ¹é…
                    terms = [token.text for token in analyzer(query_str, phrase_mode=True)]
                    if terms and len(terms) > 1:
                        strategies.append(Phrase(target_field, terms))
                        print(f"ç­–ç•¥4: åˆ†è¯çŸ­è¯­åŒ¹é… - {terms}")
                    
                    # ä½¿ç”¨ORæŸ¥è¯¢ç»„åˆæ‰€æœ‰ç­–ç•¥
                    if len(strategies) > 1:
                        text_query = Or(strategies)
                        print(f"æ„é€ ç»„åˆç²¾ç¡®æœç´¢æŸ¥è¯¢: {len(strategies)} ç§ç­–ç•¥")
                    elif len(strategies) == 1:
                        text_query = strategies[0]
                        print(f"ä½¿ç”¨å•ä¸€ç­–ç•¥ç²¾ç¡®æœç´¢")
                    else:
                        print("æ— æ³•æ„é€ ç²¾ç¡®æœç´¢æŸ¥è¯¢")
                        text_query = None
                    
                    if text_query:
                        parsed_query_obj = text_query
                        print(f"æœ€ç»ˆç²¾ç¡®æœç´¢æŸ¥è¯¢: {text_query}")
                    else:
                        print(f"ç²¾ç¡®æœç´¢æŸ¥è¯¢æ„é€ å¤±è´¥")
            elif search_mode == 'fuzzy':
                # --- ä½¿ç”¨ç»Ÿä¸€å‡½æ•°å¤„ç†å…¨æ–‡æœç´¢çš„é€šé…ç¬¦ --- 
                if '*' in query_str or '?' in query_str:
                    # --- å¤„ç†ä¸­æ–‡é€šé…ç¬¦ç‰¹æ®Šæ‰©å±•æŸ¥è¯¢ ---
                    if chinese_wildcard_expansion and expanded_queries:
                        # åˆ›å»ºå¤åˆæŸ¥è¯¢ (ORç»„åˆå¤šä¸ªé€šé…ç¬¦æŸ¥è¯¢)
                        sub_queries = []
                        for exp_query in expanded_queries:
                            sub_queries.append(Wildcard(target_field, exp_query))
                            print(f"æ·»åŠ é€šé…ç¬¦å­æŸ¥è¯¢: {exp_query}")
                        
                        if sub_queries:
                            # ä½¿ç”¨ORç»„åˆæ‰€æœ‰å­æŸ¥è¯¢
                            if len(sub_queries) == 1:
                                text_query = sub_queries[0]
                            else:
                                text_query = Or(sub_queries)
                            parsed_query_obj = text_query
                            print(f"æ„å»ºå¤åˆé€šé…ç¬¦æŸ¥è¯¢: {text_query}")
                    else:
                        print(f"Wildcard detected in fuzzy mode for '{target_field}': '{query_str}'. Constructing direct Wildcard query.")
                        processed_query = process_wildcard_query(query_str, is_filename_search=False)
                        text_query = Wildcard(target_field, processed_query)
                        parsed_query_obj = text_query
                        print(f"Constructed direct Wildcard query on '{target_field}': {text_query}")
                else: # Not a wildcard query in fuzzy mode, use QueryParser as before for keywords, etc.
                    parser = QueryParser(target_field, schema=ix.schema)
                    try:
                        parsed_q = parser.parse(query_str)
                        parsed_query_obj = parsed_q # Store parsed object before conversion
                        print(f"Parsed Keyword query on '{target_field}': {parsed_q}")
                        # Convert to prefix for fulltext search
                        text_query = convert_term_to_prefix(parsed_q, fieldname=target_field)
                        if text_query != parsed_q:
                            print(f"-> Converted terms to prefix on '{target_field}': {text_query}")
                    except Exception as e:
                        print(f"Error parsing fuzzy query on '{target_field}': {e}")
                        text_query = None
                        parsed_query_obj = None
            else:
                print(f"Error: Unknown search mode '{search_mode}' for fulltext search")
        # --- END MODIFIED --- 
    else:
        print("No text query provided.")

    size_filter_query = None
    min_bytes = min_size_kb * 1024 if min_size_kb is not None else None
    max_bytes = max_size_kb * 1024 if max_size_kb is not None else None
    if min_bytes is not None or max_bytes is not None:
        size_filter_query = NumericRange("file_size", min_bytes, max_bytes)
        print(f"Constructed Size filter query: {size_filter_query}")
    date_filter_query = None
    start_timestamp = None
    end_timestamp = None
    if start_date:
        try:
            dt_start = datetime.strptime(start_date, '%Y-%m-%d')
            start_timestamp = dt_start.timestamp()
        except ValueError as e:
            print(f"Error parsing start_date '{start_date}': {e}. Expected format: YYYY-MM-DD")
    if end_date:
        try:
            dt_end = datetime.strptime(end_date, '%Y-%m-%d')
            dt_end = dt_end.replace(hour=23, minute=59, second=59, microsecond=999999)
            end_timestamp = dt_end.timestamp()
        except ValueError as e:
            print(f"Error parsing end_date '{end_date}': {e}. Expected format: YYYY-MM-DD")
    if start_timestamp is not None or end_timestamp is not None:
        date_filter_query = NumericRange("last_modified", start_timestamp, end_timestamp)
        print(f"Constructed Date filter query: {date_filter_query}")
    file_type_query = None
    if file_type_filter:
        lower_case_filters = [ftype.lower().lstrip('.') for ftype in file_type_filter if ftype]
        if lower_case_filters:
            type_queries = [Term("file_type", ftype) for ftype in lower_case_filters]
            if len(type_queries) == 1:
                file_type_query = type_queries[0]
            else:
                file_type_query = Or(type_queries)
            print(f"Constructed File Type filter query: {file_type_query}")
        else:
            print("File type filter list was empty or contained only empty strings.")

    # Combine all queries
    all_queries = []
    if text_query:
        all_queries.append(text_query)
    if size_filter_query:
        all_queries.append(size_filter_query)
    if date_filter_query:
        all_queries.append(date_filter_query)
    if file_type_query:
        all_queries.append(file_type_query)

    final_query = None
    if not all_queries:
        # --- If only filter criteria are present, search everything --- MODIFIED
        # print("Error: No search criteria (text, size, date, or type filter) provided.")
        print("No specific text query, searching based on filters only.")
        final_query = Every() # Match all documents if no criteria, filters will apply later
    elif len(all_queries) == 1:
        final_query = all_queries[0]
    else:
        final_query = And(all_queries)
        print(f"Combined query: {final_query}")

    # Sorting logic (remains the same)
    sort_field = None
    reverse = False
    if sort_by == 'date_asc':
        sort_field = 'last_modified'
        reverse = False
    elif sort_by == 'date_desc':
        sort_field = 'last_modified'
        reverse = True
    elif sort_by == 'size_asc':
        sort_field = 'file_size'
        reverse = False
    elif sort_by == 'size_desc':
        sort_field = 'file_size'
        reverse = True
    elif sort_by == 'relevance':
        sort_field = None  # Default Whoosh scoring
    else:
        print(f"Warning: Unknown sort_by option '{sort_by}', defaulting to relevance.")
        sort_field = None

    # --- ä¿®æ”¹æœç´¢ç»“æœå¤„ç†é€»è¾‘ï¼Œè¿‡æ»¤æ‰è®¸å¯è¯æ— æ³•è®¿é—®çš„æ–‡ä»¶ç±»å‹ ---
    results = searcher.search(final_query, limit=3000, sortedby=sort_field, reverse=reverse) # Performance-balanced limit with user experience priority
    
    # --- Result Processing and Highlighting (Conditional) --- MODIFIED
    if results:
        print(f"Found {len(results)} document hit(s):")
        matched_contexts = 0
        
        # --- MODIFIED: Get positive terms for highlighting --- 
        # Only prepare for content highlighting if scope is fulltext and we have a parsed query
        positive_terms_for_highlighting = set()
        if search_scope == 'fulltext' and parsed_query_obj:
            positive_terms_for_highlighting = get_positive_terms(parsed_query_obj)
            print(f"DEBUG: Positive terms for highlighting: {positive_terms_for_highlighting}")
        # -----------------------------------------------------
        
        for hit in results:
            file_path = hit.get('path', "(æœªçŸ¥æ–‡ä»¶)")
            file_type = hit.get('file_type', '')
            
            # --- æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨ ---
            if not os.path.exists(file_path):
                print(f"Skipping result for {file_path} because file no longer exists")
                continue  # è·³è¿‡æ­¤ç»“æœï¼Œæ–‡ä»¶å·²è¢«åˆ é™¤
            # -----------------------------------------
            
            # --- æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨å½“å‰æºç›®å½•ä¸­ ---
            if current_source_dirs:
                # æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„å’Œæºç›®å½•è·¯å¾„è¿›è¡Œæ¯”è¾ƒ
                file_path_normalized = os.path.normpath(file_path).lower()
                is_in_current_dirs = False
                
                for source_dir in current_source_dirs:
                    source_dir_normalized = os.path.normpath(source_dir).lower()
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨è¿™ä¸ªæºç›®å½•æˆ–å…¶å­ç›®å½•ä¸­
                    if file_path_normalized.startswith(source_dir_normalized + os.sep) or \
                       file_path_normalized == source_dir_normalized:
                        is_in_current_dirs = True
                        break
                
                if not is_in_current_dirs:
                    print(f"Skipping result for {file_path} because it's not in current source directories")
                    continue  # è·³è¿‡æ­¤ç»“æœï¼Œä¸åœ¨å½“å‰æºç›®å½•ä¸­
            # -----------------------------------------
            
            # --- æ£€æŸ¥å½“å‰è®¸å¯è¯æ˜¯å¦å…è®¸è®¿é—®è¯¥æ–‡ä»¶ç±»å‹ ---
            if file_type:
                # æ ‡å‡†åŒ–æ–‡ä»¶ç±»å‹æ ¼å¼ï¼Œç¡®ä¿éƒ½ä»¥ç‚¹å¼€å¤´
                normalized_file_type = file_type if file_type.startswith('.') else f'.{file_type}'
                
                if normalized_file_type not in allowed_file_types:
                    print(f"Skipping result for {file_path} due to license restrictions (type: {file_type})")
                    continue  # è·³è¿‡æ­¤ç»“æœï¼Œä¸æ·»åŠ åˆ°è¿”å›åˆ—è¡¨
            # -----------------------------------------
            
            # --- Basic result structure (always included) ---
            basic_result_info = {
                'file_path': file_path,
                'last_modified': hit.get('last_modified', 0),
                'file_size': hit.get('file_size', 0),
                'file_type': file_type,
                'score': hit.score
            }
            # -----------------------------------------------
            
            # --- Content-based processing only for fulltext search ---
            if search_scope == 'fulltext' and query_str: # Only process structure if doing text search in content
                added_paragraphs_in_hit = set()
                structure = []
                structure_map_json = hit.get('structure_map')
                if structure_map_json:
                    try:
                        structure = json.loads(structure_map_json)
                    except json.JSONDecodeError:
                        print(f"Error parsing structure for {file_path}")
                        # Add the basic info even if structure fails
                        processed_results.append(basic_result_info)
                        continue
                else:
                    print(f"Warning: No structure information for {file_path}")
                    # Add the basic info if no structure
                    processed_results.append(basic_result_info)
                    continue

                found_match_in_content = False # Flag to track if any block matched
                for i, block in enumerate(structure):
                    block_text = block.get('text', '')
                    block_type = block.get('type')
                    # Initialize markers/highlights
                    final_marked_paragraph = ""
                    final_marked_heading = ""
                    final_marked_excel_values = None
                    is_relevant_block = False # Reset for each block

                    # Helper for fuzzy highlighting (now uses specific positive terms)
                    def fuzzy_highlighter(text, terms_to_highlight):
                        if not isinstance(text, str) or not terms_to_highlight:
                            return str(text)
                        temp_text = text
                        for term in terms_to_highlight:
                            def replace_func(matchobj):
                                return f"__HIGHLIGHT_START__{matchobj.group(0)}__HIGHLIGHT_END__"
                            # Use regex to match whole word unless term contains wildcard?
                            # For now, simple substring match might be okay if terms are from parser
                            temp_text = re.sub(re.escape(term), replace_func, temp_text, flags=re.IGNORECASE)
                        return temp_text
                        
                    # Helper for phrase highlighting (remains mostly the same, but should ideally use positive phrase)
                    def phrase_highlighter(text, phrase):
                        if not isinstance(text, str) or not phrase:
                            return str(text)
                        phrase_pattern = re.compile(re.escape(phrase), flags=re.IGNORECASE)
                        def replace_phrase_func(matchobj):
                            return f"__HIGHLIGHT_START__{matchobj.group(0)}__HIGHLIGHT_END__"
                        return phrase_pattern.sub(replace_phrase_func, text)

                    # Check if block is relevant based on search mode and content
                    if search_mode == 'phrase':
                        # æ·»åŠ ç²¾ç¡®æœç´¢è°ƒè¯•ä¿¡æ¯
                        escaped_query = re.escape(query_str)
                        match = re.search(escaped_query, block_text, flags=re.IGNORECASE)
                        if match:
                           print(f"ğŸ¯ ç²¾ç¡®åŒ¹é…æ‰¾åˆ°: æ–‡ä»¶ {file_path}, å—ç±»å‹ {block_type}")
                           print(f"   æŸ¥è¯¢: '{query_str}' -> æ­£åˆ™: '{escaped_query}'")
                           print(f"   åŒ¹é…æ–‡æœ¬: '{match.group(0)}'")
                           print(f"   å—å†…å®¹å‰50å­—ç¬¦: '{block_text[:50]}...'")
                           is_relevant_block = True
                           highlighted_block_text = phrase_highlighter(block_text, query_str)
                           if block_type == 'heading' or block_type == 'metadata':
                               final_marked_heading = highlighted_block_text
                           elif block_type == 'excel_row':
                               original_excel_values = block.get('values', [])
                               marked_excel_values = []
                               for cell_value in original_excel_values:
                                   marked_excel_values.append(phrase_highlighter(cell_value, query_str))
                               final_marked_excel_values = marked_excel_values
                           else: # paragraph
                               final_marked_paragraph = highlighted_block_text
                               
                    elif search_mode == 'fuzzy':
                        contains_term = False
                        # --- MODIFIED: Check against positive terms only --- 
                        if positive_terms_for_highlighting:
                            # Check block text first
                            if block_text:
                                block_text_lower = block_text.lower()
                                for term in positive_terms_for_highlighting:
                                    if term in block_text_lower:
                                        contains_term = True
                                        break
                            # If not in text, check excel values if applicable
                            if not contains_term and block_type == 'excel_row':
                                original_excel_values = block.get('values', [])
                                for cell_value in original_excel_values:
                                    if isinstance(cell_value, str):
                                        cell_lower = cell_value.lower()
                                        for term in positive_terms_for_highlighting:
                                            if term in cell_lower:
                                                contains_term = True
                                                break
                                    if contains_term:
                                        break
                        # -----------------------------------------------
                                        
                        if contains_term:
                           is_relevant_block = True
                           # --- MODIFIED: Pass positive terms to highlighter --- 
                           if block_type == 'heading' or block_type == 'metadata':
                               final_marked_heading = fuzzy_highlighter(block_text, positive_terms_for_highlighting)
                           elif block_type == 'excel_row':
                               original_excel_values = block.get('values', [])
                               marked_excel_values = []
                               for cell_value in original_excel_values:
                                   marked_excel_values.append(fuzzy_highlighter(cell_value, positive_terms_for_highlighting))
                               final_marked_excel_values = marked_excel_values
                           else: # paragraph
                               final_marked_paragraph = fuzzy_highlighter(block_text, positive_terms_for_highlighting)
                           # -------------------------------------------------
                               
                    # If the block was relevant, create a detailed result entry
                    if is_relevant_block:
                        if block_text not in added_paragraphs_in_hit: # Avoid duplicate paragraphs from same file hit
                            result_block = {
                                **basic_result_info, # Include basic info
                                'type': block_type,
                                'level': block.get('level'),
                                'paragraph': block.get('text') if block_type != 'heading' and block_type != 'metadata' and block_type != 'excel_row' else None,
                                'heading': block.get('text') if block_type == 'heading' or block_type == 'metadata' else None,
                                'excel_sheet': block.get('sheet_name'),
                                'excel_row_idx': block.get('row_index'),
                                'excel_headers': block.get('headers'),
                                'excel_values': final_marked_excel_values if final_marked_excel_values is not None else block.get('values'), # Use marked or original
                                'marked_paragraph': final_marked_paragraph,
                                'marked_heading': final_marked_heading,
                                # Keep score from basic_result_info
                            }
                            # print(f"MATCHED BLOCK: {result_block}") # Too verbose
                            processed_results.append(result_block)
                            found_match_in_content = True
                            matched_contexts += 1
                            added_paragraphs_in_hit.add(block_text)
                            
                # If after checking all blocks, no content match was found for this hit
                # (This shouldn't happen often with fulltext search, but as a fallback)
                if not found_match_in_content:
                    print(f"Note: Hit found for {file_path} but no specific content block matched/highlighted.")
                    processed_results.append(basic_result_info) # Add basic info
                    
            else: # If search_scope is 'filename' OR query_str is empty (filter only search)
                # Just add the basic file info, no content highlighting needed here
                processed_results.append(basic_result_info)
        # End of loop through hits
    else:
        print("No document hits found for the query.")
        
    # Close searcher and index
    if searcher: searcher.close()
    if ix: ix.close()
    
    # æ™ºèƒ½ç»“æœæˆªæ–­å’Œç”¨æˆ·å‹å¥½æç¤º
    original_count = len(processed_results)
    max_recommended_results = 500  # æ¨èçš„æœ€å¤§ç»“æœæ•°
    
    if original_count > max_recommended_results:
        # æˆªæ–­åˆ°æ¨èæ•°é‡ï¼Œä¿ç•™ç›¸å…³åº¦æœ€é«˜çš„ç»“æœ
        processed_results = processed_results[:max_recommended_results]
        print(f"âš ï¸  ç»“æœæ•°é‡è¾ƒå¤š ({original_count} æ¡)ï¼Œä¸ºä¿è¯æ€§èƒ½å·²æˆªæ–­åˆ° {max_recommended_results} æ¡")
        print(f"ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨æ›´å…·ä½“çš„æœç´¢è¯æˆ–æ·»åŠ ç­›é€‰æ¡ä»¶ä»¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœ")
    elif original_count > 500:
        print(f"ğŸ’¡ æ‰¾åˆ° {original_count} æ¡ç»“æœï¼Œå°†ä½¿ç”¨è™šæ‹Ÿæ»šåŠ¨æ¨¡å¼ä¿è¯ç•Œé¢æµç•…æ€§")
    
    print(f"--- Search complete. Returning {len(processed_results)} processed results. ---")
    return processed_results

# --- MODIFIED: Accept a dictionary --- 
# def _extract_worker(item_data: tuple) -> dict:
def _extract_worker(worker_args: dict) -> dict:
# -----------------------------------
    # --- Unpack arguments --- 
    # --- This is the OLD logic causing the KeyError ---
    # item_data = worker_args['item_data'] # <<< The line causing KeyError
    # enable_ocr = worker_args['enable_ocr'] # This key might also change based on how we passed it
    # path_key = item_data[0]
    # mtime = item_data[1]
    # fsize = item_data[2]
    # file_type = item_data[3]
    # archive_path = Path(item_data[4]) if len(item_data) > 4 and item_data[4] else None
    # member_name = item_data[5] if len(item_data) > 5 and item_data[5] else None
    # --- End of OLD logic ---

    # --- NEW logic to fix KeyError ---
    path_key = worker_args['path_key']
    file_type = worker_args['file_type']
    archive_path_abs_str = worker_args.get('archive_path_abs') # Use .get for optional keys
    member_name = worker_args.get('member_name')
    enable_ocr_for_file = worker_args['enable_ocr'] # Get per-file OCR setting
    # --- Get timeout value --- 
    extraction_timeout = worker_args.get('extraction_timeout') # Can be None
    # -----------------------
    # --- ADDED: Unpack content limit --- 
    content_limit_bytes = worker_args.get('content_limit_bytes', 0) # Default to 0 (no limit)
    # -----------------------------------
    # --- ADDED: Get index_dir_path for recording skipped files ---
    index_dir_path = worker_args.get('index_dir_path', '')
    # -----------------------------------------------------------
    # --- ADDED: Get cancel callback ---
    cancel_callback = worker_args.get('cancel_callback')
    # ----------------------------------
    # --- ADDED: Get filename-only flag ---
    is_filename_only = worker_args.get('is_filename_only', False)
    # ------------------------------------
    original_mtime = worker_args['original_mtime']
    original_fsize = worker_args['original_fsize']
    display_name = worker_args.get('display_name', Path(path_key).name if "::" not in path_key else path_key.split("::")[1]) # Use provided or generate
    filename_for_index = Path(path_key).name if not member_name else Path(member_name).name

    # --- Variable Initialization --- 
    text_content = None
    structure = [] # Initialize for non-PDF types
    error_message = None
    content_truncated = False # --- ADDED: Flag for truncation

    try:
        start_time = time.time()
        
        # --- MODIFIED: åœ¨å¼€å§‹å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ ---
        check_cancellation(cancel_callback, "æ–‡ä»¶æå–å¤„ç†å¼€å§‹")
        # ----------------------------------------
        
        # --- ADDED: å¤„ç†ä»…æ–‡ä»¶åç´¢å¼•æ¨¡å¼ ---
        if is_filename_only:
            print(f"ä»…æ–‡ä»¶åç´¢å¼•: {display_name}")
            return {
                'path_key': path_key,
                'text_content': '',  # ç©ºå†…å®¹
                'filename': filename_for_index,
                'structure': [],
                'mtime': original_mtime,
                'fsize': original_fsize,
                'file_type': Path(path_key).suffix.lower().lstrip('.'),
                'ocr_enabled_for_file': False,
                'display_name': display_name,
                'content_source': 'filename_only',  # æ ‡è®°æ¥æº
                'error': None,
                'content_truncated': False
            }
        # -----------------------------------
        
        # --- Select extraction function based on file type --- 
        if file_type == 'file':
            file_path = Path(path_key)
            file_ext = file_path.suffix.lower()
            
            # --- ADDED: åœ¨ç¡®å®šæ–‡ä»¶ç±»å‹åå†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
            check_cancellation(cancel_callback, f"æ–‡ä»¶ç±»å‹ç¡®å®š {file_ext}")
            # ----------------------------------------
            
            # Select function based on extension (Corrected indentation)
            if file_ext == '.docx':
                try:
                    print(f"å¼€å§‹å¤„ç†DOCXæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_docx(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†DOCXæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"DOCXæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.txt':
                # --- MODIFIED: Apply content limit to TXT --- 
                try:
                    print(f"å¼€å§‹å¤„ç†TXTæ–‡ä»¶: {display_name}")
                    text_content_tuple = extract_text_from_txt(file_path, cancel_callback) # Corrected indentation
                    if isinstance(text_content_tuple, tuple) and len(text_content_tuple) == 2:
                        text_content = text_content_tuple[0]
                        structure = text_content_tuple[1]
                        if text_content is None:
                            error_message = "TXT extraction failed."
                        elif content_limit_bytes and content_limit_bytes > 0: # Check if limit applies
                            # Encode to check byte length and truncate if needed
                            try:
                                encoded_content = text_content.encode('utf-8', errors='ignore')
                                if len(encoded_content) > content_limit_bytes:
                                    truncated_bytes = encoded_content[:content_limit_bytes]
                                    # Decode back, ignoring errors for partial characters
                                    text_content = truncated_bytes.decode('utf-8', errors='ignore')
                                    content_truncated = True # Set the flag
                                    # print(f"Info: Content truncated for {display_name} to {content_limit_bytes} bytes.") # COMMENTED OUT
                            except Exception as enc_err:
                                # print(f"Warning: Error during content truncation check for {display_name}: {enc_err}") # COMMENTED OUT
                                pass # Ignore truncation check errors silently for now
                    else:
                        error_message = "TXT extraction returned unexpected result."
                        text_content = None
                        structure = []
                    print(f"å®Œæˆå¤„ç†TXTæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"TXTæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.pdf':
                # --- MODIFIED: Add explicit timeout parameter ---
                try:
                    print(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {display_name} (OCR: {'å¯ç”¨' if enable_ocr_for_file else 'ç¦ç”¨'})")
                    
                    # --- ADDED: åœ¨å¼€å§‹PDFå¤„ç†å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
                    check_cancellation(cancel_callback, f"PDFå¤„ç†å¼€å§‹ {display_name}")
                    # ----------------------------------------
                    
                    text_content_tuple = extract_text_from_pdf(file_path, enable_ocr=enable_ocr_for_file, timeout=extraction_timeout, cancel_callback=cancel_callback)
                    if isinstance(text_content_tuple, tuple) and len(text_content_tuple) >= 2:
                        text_content = text_content_tuple[0]
                        structure = text_content_tuple[1] if text_content_tuple[1] is not None else []
                    else:
                        error_message = "PDF extraction returned unexpected result."
                        text_content = None
                        structure = []
                    print(f"å®Œæˆå¤„ç†PDFæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"PDFæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except Exception as e:
                    error_message = str(e)
                    text_content = None
                    structure = []
                    # --- ADDED: Record timeout errors for PDF extraction --- 
                    if "timeout" in error_message.lower() or "timed out" in error_message.lower():
                        if index_dir_path:
                            record_skipped_file(index_dir_path, str(file_path), f"PDFå¤„ç†è¶…æ—¶ ({extraction_timeout}ç§’)")
                    # --- ADDED: Record license errors for PDF extraction --- 
                    elif isinstance(e, PermissionError) or "è®¸å¯è¯" in error_message or "license" in error_message.lower():
                        if index_dir_path:
                            record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - PDFæ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯")
            elif file_ext == '.pptx':
                try:
                    print(f"å¼€å§‹å¤„ç†PPTXæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_pptx(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†PPTXæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"PPTXæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.xlsx':
                try:
                    print(f"å¼€å§‹å¤„ç†XLSXæ–‡ä»¶: {display_name}")
                    # ä¼ é€’å–æ¶ˆå›è°ƒç»™Excelå¤„ç†å‡½æ•°
                    text_content, structure = extract_text_from_xlsx(file_path, cancel_callback=cancel_callback)
                    print(f"å®Œæˆå¤„ç†XLSXæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"XLSXæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.md':
                try:
                    print(f"å¼€å§‹å¤„ç†MDæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_md(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†MDæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"MDæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    text_content = None
                    structure = []
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - Markdownæ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯")
                except Exception as e:
                    error_message = str(e)
                    text_content = None
                    structure = []
            elif file_ext in ('.html', '.htm'): # Corrected structure/indentation
                try:
                    print(f"å¼€å§‹å¤„ç†HTMLæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_html(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†HTMLæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"HTMLæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.rtf': # Corrected structure/indentation
                try:
                    print(f"å¼€å§‹å¤„ç†RTFæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_rtf(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†RTFæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"RTFæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - {error_message}")
            elif file_ext == '.eml': # Corrected structure/indentation
                try:
                    print(f"å¼€å§‹å¤„ç†EMLæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_eml(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†EMLæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"EMLæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    text_content = None
                    structure = []
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - é‚®ä»¶æ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯")
                except Exception as e:
                    error_message = str(e)
                    text_content = None
            elif file_ext == '.msg': # Corrected structure/indentation
                try:
                    print(f"å¼€å§‹å¤„ç†MSGæ–‡ä»¶: {display_name}")
                    text_content, structure = extract_text_from_msg(file_path, cancel_callback)
                    print(f"å®Œæˆå¤„ç†MSGæ–‡ä»¶: {display_name}")
                except InterruptedError:
                    # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    print(f"MSGæ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
                    raise
                except PermissionError as perm_err:
                    # è®°å½•å› è®¸å¯è¯é™åˆ¶è·³è¿‡çš„æ–‡ä»¶
                    error_message = str(perm_err)
                    text_content = None
                    structure = []
                    if index_dir_path:
                        record_skipped_file(index_dir_path, str(file_path), f"è®¸å¯è¯é™åˆ¶ - é‚®ä»¶æ”¯æŒåŠŸèƒ½éœ€è¦ä¸“ä¸šç‰ˆè®¸å¯è¯")
                except Exception as e:
                    error_message = str(e)
                    text_content = None
                    structure = []
            else: # Corrected structure/indentation
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šåª’ä½“æ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™ä»…ç´¢å¼•æ–‡ä»¶å
                if file_ext in FILENAME_ONLY_EXTENSIONS:
                    # å¤šåª’ä½“æ–‡ä»¶ï¼šä»…ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå†…å®¹
                    text_content = filename_for_index
                    structure = [{'type': 'filename', 'text': filename_for_index}]
                    print(f"å¤šåª’ä½“æ–‡ä»¶ä»…ç´¢å¼•æ–‡ä»¶å: {display_name}")
                else:
                    error_message = f"Unsupported file extension: {file_ext}"
                    text_content = ""
                    structure = []
        
        elif file_type == 'archive':
            # --- ADDED: åœ¨å¤„ç†å‹ç¼©åŒ…å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
            check_cancellation(cancel_callback, "å‹ç¼©åŒ…å¤„ç†å¼€å§‹")
            # ----------------------------------------
            
            if not archive_path_abs_str or not member_name:
                error_message = "Archive path or member name missing for archive extraction"
                text_content = ""
                structure = []
            else:
                archive_path = Path(archive_path_abs_str)
                member_ext = Path(member_name).suffix.lower()
                archive_type = archive_path.suffix.lower()
                # --- Extract member to temp file and process --- 
                with tempfile.TemporaryDirectory() as temp_dir:
                    temp_file_path = Path(temp_dir) / Path(member_name).name # Use just filename in temp dir
                    try: # Added try
                        # ... (Archive extraction logic remains the same) ...
                        if archive_type == '.zip': # Corrected indentation
                            try: # Try opening the zip file
                                with zipfile.ZipFile(archive_path, 'r') as zf:
                                    # --- ADDED: Try block specifically for member extraction ---
                                    try:
                                        zf.extract(member_name, path=temp_dir)
                                        # Rename logic after successful extraction
                                        extracted_member_path = Path(temp_dir) / member_name
                                        if extracted_member_path.exists() and extracted_member_path != temp_file_path:
                                            extracted_member_path.rename(temp_file_path)
                                    except RuntimeError as e_extract_member:
                                        if 'password required' in str(e_extract_member).lower() or 'encrypted file' in str(e_extract_member).lower():
                                            error_message = f"å—å¯†ç ä¿æŠ¤çš„ZIPæˆå‘˜: {member_name}"
                                            archive_member_path = f"{archive_path_abs_str}::{member_name}"
                                            record_skipped_file(
                                                index_dir_path,
                                                archive_member_path,
                                                format_skip_reason("password_zip", f"æˆå‘˜ '{member_name}' éœ€è¦å¯†ç ")
                                            )
                                        else:
                                             error_message = f"æå–ZIPæˆå‘˜æ—¶å‘ç”Ÿè¿è¡Œæ—¶é”™è¯¯ '{member_name}': {e_extract_member}"
                                             record_skipped_file(
                                                 index_dir_path,
                                                 f"{archive_path_abs_str}::{member_name}",
                                                 format_skip_reason("extraction_error", f"æˆå‘˜è¿è¡Œæ—¶é”™è¯¯: {e_extract_member}")
                                             )
                                        temp_file_path = None # Indicate extraction failed
                                    except zipfile.BadZipFile as e_bad_member:
                                         error_message = f"ZIPæˆå‘˜æŸåæˆ–æ ¼å¼é”™è¯¯: {member_name}"
                                         record_skipped_file(
                                             index_dir_path,
                                             f"{archive_path_abs_str}::{member_name}",
                                             format_skip_reason("corrupted_zip", f"æˆå‘˜æŸå: {e_bad_member}")
                                         )
                                         temp_file_path = None # Indicate extraction failed
                                    except KeyError as e_key_extract:
                                        error_message = f"ZIPæˆå‘˜æœªæ‰¾åˆ°æˆ–æ— æ³•æå–: {member_name}"
                                        record_skipped_file(
                                            index_dir_path,
                                            f"{archive_path_abs_str}::{member_name}",
                                            format_skip_reason("extraction_error", f"æˆå‘˜ '{member_name}' æœªæ‰¾åˆ°: {e_key_extract}")
                                        )
                                        temp_file_path = None # Indicate extraction failed
                                    except Exception as e_generic_extract:
                                         error_message = f"æå–ZIPæˆå‘˜æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ '{member_name}': {e_generic_extract}"
                                         record_skipped_file(
                                             index_dir_path,
                                             f"{archive_path_abs_str}::{member_name}",
                                             format_skip_reason("extraction_error", f"æˆå‘˜æå–æœªçŸ¥é”™è¯¯: {e_generic_extract}")
                                         )
                                         temp_file_path = None # Indicate extraction failed
                                    # --- END ADDED Try block ---

                            # --- Existing handling for errors opening the main ZIP ---
                            except (zipfile.BadZipFile, RuntimeError) as e_open_zip:
                                reason_key = "password_zip" if 'password required' in str(e_open_zip).lower() else "corrupted_zip"
                                error_message = f"{format_skip_reason(reason_key)}: {archive_path.name}"
                                record_skipped_file(
                                    index_dir_path,
                                    str(archive_path), # Record the archive path itself
                                    format_skip_reason(reason_key, str(e_open_zip))
                                )
                                temp_file_path = None # Indicate extraction failed
                            # --- End Existing Handling ---

                        elif archive_type == '.rar': # Corrected indentation
                            with rarfile.RarFile(archive_path, 'r') as rf:
                                rf.extract(member_name, path=temp_dir)
                                extracted_member_path = Path(temp_dir) / member_name
                                if extracted_member_path.exists() and extracted_member_path != temp_file_path:
                                    extracted_member_path.rename(temp_file_path)
                        else: # Corrected indentation
                            error_message = f"Unsupported archive type: {archive_type}"
                        
                        # --- ADDED: åœ¨æå–å®Œæˆåæ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
                        check_cancellation(cancel_callback, "å‹ç¼©åŒ…æˆå‘˜æå–å®Œæˆ")
                        # ----------------------------------------
                        
                        if not error_message and temp_file_path and temp_file_path.exists():
                            # --- Now extract text from the temporary file --- 
                            if member_ext == '.docx': # Corrected indentation
                                text_content, structure = extract_text_from_docx(temp_file_path, cancel_callback)
                            elif member_ext == '.txt':
                                # --- MODIFIED: Apply content limit to TXT member --- 
                                text_content_tuple = extract_text_from_txt(temp_file_path, cancel_callback)
                                if isinstance(text_content_tuple, tuple) and len(text_content_tuple) == 2:
                                    text_content = text_content_tuple[0]
                                    structure = text_content_tuple[1]
                                    if text_content is None:
                                        error_message = "TXT member extraction failed."
                                    elif content_limit_bytes and content_limit_bytes > 0: # Check if limit applies
                                        # Encode to check byte length and truncate if needed
                                        try:
                                            encoded_content = text_content.encode('utf-8', errors='ignore')
                                            if len(encoded_content) > content_limit_bytes:
                                                truncated_bytes = encoded_content[:content_limit_bytes]
                                                # Decode back, ignoring errors for partial characters
                                                text_content = truncated_bytes.decode('utf-8', errors='ignore')
                                                content_truncated = True # Set the flag
                                                # print(f"Info: Content truncated for archive member {display_name} to {content_limit_bytes} bytes.") # COMMENTED OUT
                                        except Exception as enc_err:
                                            # print(f"Warning: Error during content truncation check for archive member {display_name}: {enc_err}") # COMMENTED OUT
                                            pass # Ignore truncation check errors silently for now
                                    else:
                                        error_message = "TXT member extraction returned unexpected result."
                                        text_content = None
                                        structure = []
                                # --- END MODIFIED --- 
                            elif member_ext == '.pdf':
                                # --- MODIFIED PDF Call for Archive Member --- 
                                pdf_result_member = extract_text_from_pdf(temp_file_path, enable_ocr=enable_ocr_for_file, timeout=extraction_timeout, cancel_callback=cancel_callback)
                                text_content = pdf_result_member[0]
                                structure = pdf_result_member[1]
                                if text_content is None:
                                    error_message = "PDF member extraction failed or timed out"
                                    # --- NEW: Record skipped file for PDF timeout ---
                                    if index_dir_path:
                                        archive_member_path = f"{archive_path_abs_str}::{member_name}"
                                        record_skipped_file(
                                            index_dir_path,
                                            archive_member_path,
                                            format_skip_reason("pdf_timeout", "PDFå¤„ç†è¶…æ—¶æˆ–è½¬æ¢é”™è¯¯")
                                        )
                                    # -------------------------------------------------
                                elif content_limit_bytes and content_limit_bytes > 0:
                                    # Apply content limit to PDF text if needed
                                    try:
                                        encoded_content = text_content.encode('utf-8', errors='ignore')
                                        if len(encoded_content) > content_limit_bytes:
                                            truncated_bytes = encoded_content[:content_limit_bytes]
                                            # Decode back, ignoring errors for partial characters
                                            text_content = truncated_bytes.decode('utf-8', errors='ignore')
                                            content_truncated = True # Set the flag
                                            # print(f"Info: PDF content truncated for archive member {display_name} to {content_limit_bytes} bytes.") # COMMENTED OUT
                                            # --- NEW: Record skipped file for content limit ---
                                            if index_dir_path:
                                                archive_member_path = f"{archive_path_abs_str}::{member_name}"
                                                record_skipped_file(
                                                    index_dir_path,
                                                    archive_member_path,
                                                    format_skip_reason("content_limit", f"å†…å®¹å¤§å°({len(encoded_content) // 1024}KB)è¶…è¿‡é™åˆ¶({content_limit_bytes // 1024}KB)")
                                                )
                                            # -------------------------------------------------
                                    except Exception as enc_err:
                                        # print(f"Warning: Error during PDF content truncation check for {display_name}: {enc_err}") # COMMENTED OUT
                                        pass # Ignore truncation check errors silently for now
                                # -------------------------------------------
                            elif member_ext == '.pptx':
                                text_content, structure = extract_text_from_pptx(temp_file_path, cancel_callback)
                            elif member_ext == '.xlsx':
                                # ä¼ é€’å–æ¶ˆå›è°ƒç»™Excelå¤„ç†å‡½æ•°
                                text_content, structure = extract_text_from_xlsx(temp_file_path, cancel_callback=cancel_callback)
                            elif member_ext == '.md':
                                text_content, structure = extract_text_from_md(temp_file_path, cancel_callback)
                            elif member_ext in ('.html', '.htm'): # Corrected structure/indentation
                                text_content, structure = extract_text_from_html(temp_file_path, cancel_callback)
                            elif member_ext == '.rtf':
                                text_content, structure = extract_text_from_rtf(temp_file_path, cancel_callback)
                            elif member_ext == '.eml':
                                text_content, structure = extract_text_from_eml(temp_file_path, cancel_callback)
                            elif member_ext == '.msg':
                                text_content, structure = extract_text_from_msg(temp_file_path, cancel_callback)
                            else: # Corrected indentation
                                # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šåª’ä½“æ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™ä»…ç´¢å¼•æ–‡ä»¶å
                                if member_ext in FILENAME_ONLY_EXTENSIONS:
                                    # å¤šåª’ä½“æ–‡ä»¶ï¼šä»…ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå†…å®¹
                                    text_content = Path(member_name).name
                                    structure = [{'type': 'filename', 'text': Path(member_name).name}]
                                    print(f"å‹ç¼©åŒ…å†…å¤šåª’ä½“æ–‡ä»¶ä»…ç´¢å¼•æ–‡ä»¶å: {member_name}")
                                else:
                                    error_message = f"Unsupported file extension in archive: {member_ext}"
                                    text_content = ""
                                    structure = []
                        elif not error_message:
                            error_message = "Failed to extract member from archive"
                            text_content = "" # Ensure defined
                            structure = []    # Ensure defined
                    # Moved exception handling outside the if/elif chain for member types
                    except FileNotFoundError:
                        error_message = f"Member not found in archive: {member_name}"
                        text_content = ""
                        structure = []
                    except zipfile.BadZipFile:
                        error_message = f"Bad ZIP file containing member: {member_name}"
                        text_content = ""
                        structure = []
                    except rarfile.BadRarFile:
                        error_message = f"Bad RAR file containing member: {member_name}"
                        text_content = ""
                        structure = []
                    except NotImplementedError as nie:
                         error_message = f"Feature not implemented for member '{member_name}': {nie}"
                         text_content = ""
                         structure = []
                    except Exception as e_extract:
                        error_message = f"Error extracting member '{member_name}': {e_extract}"
                        # print(traceback.format_exc(), file=sys.stderr) # Already commented? Ensure commented.
                        text_content = ""
                        structure = []
        else:
            error_message = f"Unknown file type for extraction: {file_type}"
            text_content = ""
            structure = []
        
        end_time = time.time()
        # print(f"Extraction time for {display_name}: {end_time - start_time:.2f}s")
    
    except InterruptedError:
        # --- MODIFIED: æ”¹è¿›å–æ¶ˆå¼‚å¸¸å¤„ç† ---
        print(f"æ–‡ä»¶å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ: {display_name}")
        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        raise
    except Exception as e_outer:
        error_message = f"Unexpected error during extraction setup for {display_name}: {e_outer}"
        # traceback.print_exc() # COMMENTED OUT
        text_content = None
        structure = []

    # --- Ensure content is string, even if empty ---
    final_content = text_content if text_content is not None else ""
    # If there was an error OR text is None, ensure structure is empty.
    final_structure = structure if error_message is None and text_content is not None else []

    # --- Construct result dictionary ---
    result = {
        'path_key': path_key,
        'display_name': display_name,
        'text_content': final_content,
        'structure': final_structure,
        'error': error_message,
        'mtime': original_mtime,
        'fsize': original_fsize,
        'file_type': Path(path_key.split('::')[0]).suffix.lower() if "::" not in path_key else Path(path_key.split('::')[1]).suffix.lower(),
        'filename': filename_for_index,
        'ocr_enabled_for_file': enable_ocr_for_file,
        'content_truncated': content_truncated
    }
    return result

# --- ADDED: Function to read license-skipped files ---
def read_license_skipped_files(index_dir_path: str) -> dict:
    """
    è¯»å–å› è®¸å¯è¯é™åˆ¶è€Œè¢«è·³è¿‡çš„æ–‡ä»¶è®°å½•
    
    Args:
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        
    Returns:
        dict: åŒ…å«è¢«è·³è¿‡æ–‡ä»¶è·¯å¾„çš„å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶è·¯å¾„ï¼Œå€¼ä¸ºè·³è¿‡åŸå› 
    """
    skipped_files = {}
    log_file_path = os.path.join(index_dir_path, "index_skipped_files.tsv")
    
    if not os.path.exists(log_file_path):
        print(f"è·³è¿‡æ–‡ä»¶è®°å½•ä¸å­˜åœ¨: {log_file_path}")
        return skipped_files
        
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f, delimiter='\t')
            # è·³è¿‡è¡¨å¤´
            next(reader, None)
            
            for row in reader:
                if len(row) >= 3:
                    file_path, reason, timestamp = row[0], row[1], row[2]
                    # ä»…å…³æ³¨å› è®¸å¯è¯é™åˆ¶è€Œè·³è¿‡çš„æ–‡ä»¶
                    if "è®¸å¯è¯é™åˆ¶" in reason:
                        skipped_files[file_path] = reason
                        
        print(f"è¯»å–äº† {len(skipped_files)} ä¸ªå› è®¸å¯è¯é™åˆ¶è€Œè·³è¿‡çš„æ–‡ä»¶è®°å½•")
    except Exception as e:
        print(f"è¯»å–è·³è¿‡æ–‡ä»¶è®°å½•æ—¶å‡ºé”™: {e}")
    
    return skipped_files
# ---------------------------------------------------

# --- ADDED: Function to update skipped files record ---
def update_skipped_files_record(index_dir_path: str, processed_license_files: dict):
    """
    æ›´æ–°è·³è¿‡æ–‡ä»¶è®°å½•ï¼Œåˆ é™¤å·²ç»é‡æ–°å¤„ç†çš„è®¸å¯è¯é™åˆ¶æ–‡ä»¶è®°å½•
    
    Args:
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        processed_license_files: å·²å¤„ç†çš„è®¸å¯è¯é™åˆ¶æ–‡ä»¶å­—å…¸
    """
    if not processed_license_files:
        return  # æ²¡æœ‰éœ€è¦æ›´æ–°çš„è®°å½•
        
    log_file_path = os.path.join(index_dir_path, "index_skipped_files.tsv")
    if not os.path.exists(log_file_path):
        return  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°
        
    try:
        # è¯»å–æ‰€æœ‰è®°å½•
        all_records = []
        with open(log_file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f, delimiter='\t')
            # ä¿å­˜è¡¨å¤´
            header = next(reader, None)
            if not header:
                return  # ç©ºæ–‡ä»¶æˆ–åªæœ‰è¡¨å¤´
                
            all_records.append(header)
            
            # è¯»å–å¹¶è¿‡æ»¤è®°å½•
            for row in reader:
                if len(row) >= 3:
                    file_path, reason, timestamp = row[0], row[1], row[2]
                    # å¦‚æœä¸æ˜¯å·²å¤„ç†çš„è®¸å¯è¯é™åˆ¶æ–‡ä»¶ï¼Œåˆ™ä¿ç•™
                    if file_path not in processed_license_files:
                        all_records.append(row)
        
        # å†™å›è¿‡æ»¤åçš„è®°å½•
        with open(log_file_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f, delimiter='\t')
            for row in all_records:
                writer.writerow(row)
                
        print(f"å·²ä»è·³è¿‡æ–‡ä»¶è®°å½•ä¸­åˆ é™¤ {len(processed_license_files)} ä¸ªå·²å¤„ç†çš„è®¸å¯è¯é™åˆ¶æ–‡ä»¶")
    except Exception as e:
        print(f"æ›´æ–°è·³è¿‡æ–‡ä»¶è®°å½•æ—¶å‡ºé”™: {e}")
# ------------------------------------------------------

# --- ç´¢å¼•é€Ÿåº¦ä¼˜åŒ–å‡½æ•° ---

def get_optimal_worker_count(task_type="io_intensive"):
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæœ€ä¼˜çš„å·¥ä½œè¿›ç¨‹æ•°é‡

    Args:
        task_type: ä»»åŠ¡ç±»å‹ï¼Œ"io_intensive"ï¼ˆI/Oå¯†é›†å‹ï¼‰æˆ–"cpu_intensive"ï¼ˆCPUå¯†é›†å‹ï¼‰

    Returns:
        int: æ¨èçš„å·¥ä½œè¿›ç¨‹æ•°é‡
    """
    cpu_count = multiprocessing.cpu_count()

    if task_type == "io_intensive":
        # I/Oå¯†é›†å‹ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤šè¿›ç¨‹
        optimal_count = min(cpu_count * 2, 8)  # æœ€å¤š8ä¸ªè¿›ç¨‹
    else:
        # CPUå¯†é›†å‹ä»»åŠ¡ï¼Œä½¿ç”¨CPUæ ¸å¿ƒæ•°
        optimal_count = max(cpu_count - 1, 1)  # ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ

    print(f"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: {cpu_count}, ä»»åŠ¡ç±»å‹: {task_type}, æ¨èå·¥ä½œè¿›ç¨‹æ•°: {optimal_count}")
    return optimal_count

def should_skip_large_file(file_path: Path, max_size_mb: int = 100) -> tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤§æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼‰

    Returns:
        tuple[bool, str]: (æ˜¯å¦è·³è¿‡, è·³è¿‡åŸå› )
    """
    try:
        file_size = file_path.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024

        if file_size > max_size_bytes:
            return True, f"æ–‡ä»¶å¤§å° ({file_size // (1024*1024)}MB) è¶…è¿‡é™åˆ¶ ({max_size_mb}MB)"

        return False, ""
    except Exception as e:
        return True, f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {e}"

def should_skip_system_file(file_path: Path) -> tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ç³»ç»Ÿæ–‡ä»¶æˆ–ä¸´æ—¶æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        tuple[bool, str]: (æ˜¯å¦è·³è¿‡, è·³è¿‡åŸå› )
    """
    try:
        path_str = str(file_path).lower()

        # è·³è¿‡çš„è·¯å¾„æ¨¡å¼
        skip_patterns = [
            'recycle.bin',
            '$recycle.bin',
            'system volume information',
            'pagefile.sys',
            'hiberfil.sys',
            'swapfile.sys',
            '.tmp',
            '.temp',
            '~$',  # Officeä¸´æ—¶æ–‡ä»¶
        ]

        for pattern in skip_patterns:
            if pattern in path_str:
                return True, f"ç³»ç»Ÿæ–‡ä»¶æˆ–ä¸´æ—¶æ–‡ä»¶: {pattern}"

        # æ£€æŸ¥æ˜¯å¦ä¸ºéšè—æ–‡ä»¶ï¼ˆWindowsï¼‰
        if file_path.name.startswith('.') and len(file_path.name) > 1:
            return True, "éšè—æ–‡ä»¶"

        return False, ""
    except Exception as e:
        return True, f"æ£€æŸ¥ç³»ç»Ÿæ–‡ä»¶æ—¶å‡ºé”™: {e}"

def scan_documents_optimized(directory_paths: list, max_file_size_mb: int = 100, 
                           skip_system_files: bool = True, file_types_to_index=None, 
                           filename_only_types=None, cancel_callback=None) -> tuple[list[Path], list[Path], list[dict]]:
    """
    ä¼˜åŒ–çš„æ–‡æ¡£æ‰«æå‡½æ•°ï¼Œæ”¯æŒå¤šä¸ªç›®å½•å’Œæ–‡ä»¶è¿‡æ»¤

    Args:
        directory_paths: è¦æ‰«æçš„ç›®å½•è·¯å¾„åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡ï¼‰
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼‰
        skip_system_files: æ˜¯å¦è·³è¿‡ç³»ç»Ÿæ–‡ä»¶
        file_types_to_index: è¦å®Œæ•´ç´¢å¼•çš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚['txt', 'docx']
        filename_only_types: åªç´¢å¼•æ–‡ä»¶åçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚['pdf', 'xlsx']

    Returns:
        tuple[list[Path], list[Path], list[dict]]: (å®Œæ•´ç´¢å¼•æ–‡ä»¶åˆ—è¡¨, ä»…æ–‡ä»¶åç´¢å¼•æ–‡ä»¶åˆ—è¡¨, è·³è¿‡çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨)
    """
    found_files = []  # éœ€è¦å®Œæ•´ç´¢å¼•çš„æ–‡ä»¶
    filename_only_files = []  # ä»…ç´¢å¼•æ–‡ä»¶åçš„æ–‡ä»¶
    skipped_files = []
    
    # ç”¨äºå»é‡çš„é›†åˆï¼Œé˜²æ­¢é‡å¤æ·»åŠ åŒä¸€æ–‡ä»¶
    processed_paths = set()  # å­˜å‚¨å·²å¤„ç†çš„è§„èŒƒåŒ–è·¯å¾„

    # è½¬æ¢ä¸ºPathå¯¹è±¡
    path_objects = []
    for dir_path in directory_paths:
        if isinstance(dir_path, str):
            path_objects.append(Path(dir_path))
        else:
            path_objects.append(dir_path)

    # ç¡®å®šå…è®¸çš„æ–‡ä»¶æ‰©å±•å
    if file_types_to_index is not None:
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†æ–‡ä»¶ç±»å‹ï¼ˆåŒ…æ‹¬ç©ºåˆ—è¡¨ï¼‰
        allowed_extensions = []
        for file_type in file_types_to_index:
            # ç¡®ä¿æ‰©å±•åä»¥ç‚¹å¼€å¤´
            ext = file_type if file_type.startswith('.') else f'.{file_type}'
            allowed_extensions.append(ext.lower())
        print(f"æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œå®Œæ•´ç´¢å¼•ä»¥ä¸‹æ–‡ä»¶ç±»å‹: {allowed_extensions}")
    else:
        # ä½¿ç”¨é»˜è®¤çš„æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        allowed_extensions = ALLOWED_EXTENSIONS
        print(f"ä½¿ç”¨é»˜è®¤æ–‡ä»¶ç±»å‹: {allowed_extensions}")
    
    # ç¡®å®šä»…æ–‡ä»¶åç´¢å¼•çš„æ–‡ä»¶æ‰©å±•å
    filename_only_extensions = []
    if filename_only_types:
        for file_type in filename_only_types:
            # ç¡®ä¿æ‰©å±•åä»¥ç‚¹å¼€å¤´
            ext = file_type if file_type.startswith('.') else f'.{file_type}'
            filename_only_extensions.append(ext.lower())
        print(f"æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œä»…æ–‡ä»¶åç´¢å¼•ä»¥ä¸‹æ–‡ä»¶ç±»å‹: {filename_only_extensions}")

    for directory_path in path_objects:
        # åœ¨æ‰«ææ¯ä¸ªç›®å½•å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, f"æ‰«æç›®å½• {directory_path}")
            
        if not directory_path.is_dir():
            print(f"é”™è¯¯: è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
            continue
            
        print(f"æ‰«æç›®å½•: {directory_path}")

        try:
            file_count = 0
            for item in directory_path.rglob('*'):
                # æ¯æ‰«æ50ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€
                file_count += 1
                periodic_cancellation_check(cancel_callback, 50, file_count, "æ–‡ä»¶æ‰«æ")
                    
                if not item.is_file():
                    continue

                # è§„èŒƒåŒ–æ–‡ä»¶è·¯å¾„ç”¨äºå»é‡æ£€æŸ¥
                normalized_path = normalize_path_for_index(str(item))
                if normalized_path in processed_paths:
                    # è·³è¿‡é‡å¤æ–‡ä»¶
                    print(f"è·³è¿‡é‡å¤æ–‡ä»¶: {item.name} (è·¯å¾„: {normalized_path})")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åçš„å¤„ç†ç­–ç•¥
                file_ext = item.suffix.lower()
                
                if file_ext in allowed_extensions:
                    # å®Œæ•´ç´¢å¼•
                    file_category = "full_index"
                elif file_ext in filename_only_extensions:
                    # ä»…æ–‡ä»¶åç´¢å¼•
                    file_category = "filename_only"
                else:
                    # è·³è¿‡æ­¤æ–‡ä»¶
                    if file_types_to_index or filename_only_types:
                        skipped_files.append({
                            'path': str(item),
                            'reason': f'æ–‡ä»¶ç±»å‹ {item.suffix} æœªè¢«é€‰æ‹©ç´¢å¼•',
                            'type': 'file_type_not_selected'
                        })
                    continue

                # æ£€æŸ¥æ˜¯å¦è·³è¿‡å¤§æ–‡ä»¶
                should_skip_large, large_reason = should_skip_large_file(item, max_file_size_mb)
                if should_skip_large:
                    skipped_files.append({
                        'path': str(item),
                        'reason': large_reason,
                        'type': 'large_file'
                    })
                    continue
                            
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡ç³»ç»Ÿæ–‡ä»¶
                if skip_system_files:
                    should_skip_sys, sys_reason = should_skip_system_file(item)
                    if should_skip_sys:
                        skipped_files.append({
                            'path': str(item),
                            'reason': sys_reason,
                            'type': 'system_file'
                        })
                        continue
                
                # æ ¹æ®æ–‡ä»¶ç±»åˆ«æ·»åŠ åˆ°ç›¸åº”åˆ—è¡¨
                if file_category == "full_index":
                    found_files.append(item)
                    processed_paths.add(normalized_path)  # è®°å½•å·²å¤„ç†çš„è·¯å¾„
                elif file_category == "filename_only":
                    filename_only_files.append(item)
                    processed_paths.add(normalized_path)  # è®°å½•å·²å¤„ç†çš„è·¯å¾„

        except InterruptedError:
            # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
            raise
        except Exception as e:
            print(f"æ‰«æç›®å½•æ—¶å‡ºé”™ {directory_path}: {e}")
            continue

    print(f"æ‰«æå®Œæˆ. å®Œæ•´ç´¢å¼•: {len(found_files)} ä¸ªæ–‡æ¡£, ä»…æ–‡ä»¶åç´¢å¼•: {len(filename_only_files)} ä¸ªæ–‡æ¡£, è·³è¿‡: {len(skipped_files)} ä¸ªæ–‡ä»¶")
    return found_files, filename_only_files, skipped_files

def estimate_processing_time(files: list[Path]) -> dict:
    """
    æ ¹æ®æ–‡ä»¶å¤§å°å’Œç±»å‹ä¼°ç®—å¤„ç†æ—¶é—´

    Args:
        files: æ–‡ä»¶åˆ—è¡¨

    Returns:
        dict: åŒ…å«æ—¶é—´ä¼°ç®—ä¿¡æ¯çš„å­—å…¸
    """
    # ä¸åŒæ–‡ä»¶ç±»å‹çš„å¤„ç†é€Ÿåº¦ä¼°ç®—ï¼ˆMB/ç§’ï¼‰
    processing_speeds = {
        '.txt': 50,   # æ–‡æœ¬æ–‡ä»¶å¤„ç†å¾ˆå¿«
        '.docx': 10,  # Wordæ–‡æ¡£ä¸­ç­‰é€Ÿåº¦
        '.pdf': 5,    # PDFæ–‡ä»¶è¾ƒæ…¢ï¼ˆç‰¹åˆ«æ˜¯OCRï¼‰
        '.pptx': 8,   # PowerPointä¸­ç­‰é€Ÿåº¦
        '.xlsx': 12,  # Excelä¸­ç­‰é€Ÿåº¦
        '.html': 20,  # HTMLæ–‡ä»¶è¾ƒå¿«
        '.md': 30,    # Markdownæ–‡ä»¶å¾ˆå¿«
        '.rtf': 15,   # RTFæ–‡ä»¶ä¸­ç­‰é€Ÿåº¦
        '.eml': 8,    # é‚®ä»¶æ–‡ä»¶ä¸­ç­‰é€Ÿåº¦
        '.msg': 6,    # Outlooké‚®ä»¶æ–‡ä»¶è¾ƒæ…¢
    }

    total_size = 0
    estimated_time = 0
    file_type_counts = {}

    for file_path in files:
        try:
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            file_ext = file_path.suffix.lower()

            total_size += file_size
            file_type_counts[file_ext] = file_type_counts.get(file_ext, 0) + 1

            # è·å–å¤„ç†é€Ÿåº¦ï¼Œé»˜è®¤å€¼ä¸º5MB/s
            speed = processing_speeds.get(file_ext, 5)
            estimated_time += file_size / speed

        except Exception:
            # å¦‚æœæ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
            estimated_time += 0.1  # å‡è®¾0.1ç§’

    return {
        'total_files': len(files),
        'total_size_mb': total_size,
        'estimated_time_seconds': estimated_time,
        'estimated_time_formatted': f"{int(estimated_time // 60)}åˆ†{int(estimated_time % 60)}ç§’",
        'file_type_counts': file_type_counts
    }

def create_or_update_index(directories: list[str], index_dir_path: str, enable_ocr: bool = True,
                          extraction_timeout: int = 300, content_limit_kb: int = 1024,
                          max_file_size_mb: int = 100, skip_system_files: bool = True,
                          incremental: bool = True, max_workers: int = None, 
                          cancel_callback=None, file_types_to_index=None, 
                          filename_only_types=None, preserve_removed_dirs: bool = True):
    """
    åˆ›å»ºæˆ–æ›´æ–°æ–‡æ¡£ç´¢å¼•ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        directories: è¦ç´¢å¼•çš„ç›®å½•åˆ—è¡¨
        index_dir_path: ç´¢å¼•å­˜å‚¨ç›®å½•
        enable_ocr: æ˜¯å¦å¯ç”¨OCR
        extraction_timeout: æ–‡ä»¶æå–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        content_limit_kb: å†…å®¹å¤§å°é™åˆ¶ï¼ˆKBï¼‰
        max_file_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼‰
        skip_system_files: æ˜¯å¦è·³è¿‡ç³»ç»Ÿæ–‡ä»¶
        incremental: æ˜¯å¦å¯ç”¨å¢é‡ç´¢å¼•
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        cancel_callback: å–æ¶ˆæ£€æŸ¥å›è°ƒå‡½æ•°ï¼Œå¦‚æœè¿”å›Trueåˆ™å–æ¶ˆæ“ä½œ
        file_types_to_index: è¦ç´¢å¼•çš„æ–‡ä»¶ç±»å‹åˆ—è¡¨ï¼Œå¦‚['txt', 'docx', 'pdf']

    Yields:
        dict: è¿›åº¦ä¿¡æ¯
    """
    # --- MODIFIED: åœ¨å‡½æ•°å¼€å§‹å°±åˆå§‹åŒ–progresså˜é‡ ---
    progress = {
        'stage': 'initializing',
        'current': 0,
        'total': 0,
        'message': 'åˆå§‹åŒ–ç´¢å¼•æ“ä½œ...',
        'files_processed': 0,
        'files_skipped': 0,
        'errors': 0
    }
    # ------------------------------------------------
    
    try:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "ç´¢å¼•åˆå§‹åŒ–")
            
        # ç¡®ä¿ç´¢å¼•ç›®å½•å­˜åœ¨
        index_path = Path(index_dir_path)
        index_path.mkdir(parents=True, exist_ok=True)

        # æ›´æ–°è¿›åº¦ä¿¡æ¯
        progress.update({
            'stage': 'scanning',
            'message': 'å¼€å§‹æ‰«ææ–‡ä»¶...'
        })
        yield progress

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "å¼€å§‹æ–‡ä»¶æ‰«æ")

        # 1. æ‰«ææ–‡ä»¶
        print("å¼€å§‹æ‰«ææ–‡æ¡£...")
        all_files, filename_only_files, skipped_files = scan_documents_optimized(
            directories, max_file_size_mb, skip_system_files, file_types_to_index, filename_only_types, cancel_callback
        )

        total_files = len(all_files) + len(filename_only_files)
        progress.update({
            'stage': 'scanning_complete',
            'total': total_files,
            'message': f'æ‰«æå®Œæˆï¼Œå®Œæ•´ç´¢å¼•: {len(all_files)} ä¸ªï¼Œä»…æ–‡ä»¶å: {len(filename_only_files)} ä¸ªï¼Œè·³è¿‡: {len(skipped_files)} ä¸ªæ–‡ä»¶',
            'files_skipped': len(skipped_files)
        })
        yield progress

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        check_cancellation(cancel_callback, "æ‰«æå®Œæˆåæ£€æŸ¥")

        if not all_files and not filename_only_files:
            progress.update({
                'stage': 'complete',
                'message': 'æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç´¢å¼•çš„æ–‡ä»¶'
            })
            yield progress
            return

        # 2. åŠ è½½æ–‡ä»¶ç¼“å­˜ï¼ˆç”¨äºå¢é‡ç´¢å¼•ï¼‰
        file_cache = {}
        # åˆå¹¶æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶
        files_to_process = all_files + filename_only_files

        if incremental:
            progress.update({
                'stage': 'change_detection',
                'message': 'æ£€æµ‹æ–‡ä»¶å˜æ›´...'
            })
            yield progress

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_callback and cancel_callback():
                raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")

            file_cache = load_file_index_cache(index_dir_path)
            new_files, modified_files, deleted_files = detect_file_changes(all_files, file_cache, filename_only_files)

            files_to_process = new_files + modified_files

            progress.update({
                'stage': 'change_detection_complete',
                'total': len(files_to_process),
                'message': f'å¢é‡æ£€æµ‹å®Œæˆ: {len(new_files)} ä¸ªæ–°æ–‡ä»¶, {len(modified_files)} ä¸ªä¿®æ”¹æ–‡ä»¶, {len(deleted_files)} ä¸ªåˆ é™¤æ–‡ä»¶'
            })
            yield progress

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_callback and cancel_callback():
                raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")

            # å¦‚æœæ²¡æœ‰å˜æ›´ï¼Œç›´æ¥è¿”å›
            if not files_to_process and not deleted_files:
                progress.update({
                    'stage': 'complete',
                    'message': 'æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œç´¢å¼•å·²æ˜¯æœ€æ–°'
                })
                yield progress
                return

        # 3. ä¼°ç®—å¤„ç†æ—¶é—´
        estimated_time_info = estimate_processing_time(files_to_process)
        progress.update({
            'stage': 'processing_start',
            'message': f'å¼€å§‹å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶ï¼Œé¢„è®¡éœ€è¦ {estimated_time_info["estimated_time_formatted"]}'
        })
        yield progress

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        if cancel_callback and cancel_callback():
            raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")

        # 4. å‡†å¤‡Whooshç´¢å¼•
        from whoosh import fields, index

        # å®šä¹‰ç´¢å¼•æ¨¡å¼
        schema = fields.Schema(
            path=fields.ID(stored=True, unique=True),
            content=fields.TEXT(stored=True),
            filename_text=fields.TEXT(stored=True),
            structure_map=fields.TEXT(stored=True),
            last_modified=fields.NUMERIC(stored=True),
            file_size=fields.NUMERIC(stored=True),
            file_type=fields.TEXT(stored=True),
            indexed_with_ocr=fields.BOOLEAN(stored=True)
        )

        # åˆ›å»ºæˆ–æ‰“å¼€ç´¢å¼•
        if index.exists_in(index_dir_path):
            ix = index.open_dir(index_dir_path)
        else:
            ix = index.create_in(index_dir_path, schema)

        # 5. æ‰¹é‡å¤„ç†æ–‡ä»¶
        if files_to_process:
            progress.update({
                'stage': 'extracting',
                'message': 'å¼€å§‹æå–æ–‡ä»¶å†…å®¹...'
            })
            yield progress

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_callback and cancel_callback():
                raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")

            # å‡†å¤‡å·¥ä½œè¿›ç¨‹å‚æ•° - åªå¤„ç†éœ€è¦æ›´æ–°çš„æ–‡ä»¶
            # åˆ†ç¦»éœ€è¦å¤„ç†çš„å®Œæ•´ç´¢å¼•æ–‡ä»¶å’Œä»…æ–‡ä»¶åç´¢å¼•æ–‡ä»¶
            files_to_process_full = [f for f in files_to_process if f in all_files]
            files_to_process_filename_only = [f for f in files_to_process if f in filename_only_files]
            
            print(f"å¢é‡å¤„ç†: éœ€è¦å…¨æ–‡ç´¢å¼• {len(files_to_process_full)} ä¸ªæ–‡ä»¶, éœ€è¦æ–‡ä»¶åç´¢å¼• {len(files_to_process_filename_only)} ä¸ªæ–‡ä»¶")
            
            worker_args_list = prepare_worker_arguments_batch(
                files_to_process_full, enable_ocr, extraction_timeout, 
                content_limit_kb, index_dir_path, files_to_process_filename_only, cancel_callback
            )

            # ç”¨äºæ”¶é›†è¿›åº¦æ›´æ–°çš„åˆ—è¡¨
            progress_updates = []

            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            def extraction_progress_callback(current, total, detail):
                progress_updates.append({
                    'stage': 'extracting',
                    'current': current,
                    'total': total,
                    'message': detail
                })

            # å¤šè¿›ç¨‹æå–å†…å®¹ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
            extraction_results = []
            total_files = len(worker_args_list)
            
            # é€ä¸ªå¤„ç†æ–‡ä»¶å¹¶å®æ—¶å‘é€è¿›åº¦
            processed_count = 0
            real_processing_total = len(files_to_process_full) + len(files_to_process_filename_only)
            
            for i, args in enumerate(worker_args_list):
                # --- MODIFIED: åœ¨æ¯ä¸ªæ–‡ä»¶å¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ ---
                if cancel_callback and cancel_callback():
                    print(f"ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œåœæ­¢å¤„ç†å‰©ä½™ {len(worker_args_list) - i} ä¸ªæ–‡ä»¶")
                    raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                # ------------------------------------------------
                    
                try:
                    # --- ADDED: åœ¨å¼€å§‹å¤„ç†å•ä¸ªæ–‡ä»¶å‰å†æ¬¡æ£€æŸ¥ ---
                    if cancel_callback and cancel_callback():
                        print(f"ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œè·³è¿‡æ–‡ä»¶: {args.get('display_name', 'unknown')}")
                        raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                    # ----------------------------------------
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£éœ€è¦å¤„ç†çš„æ–‡ä»¶
                    file_path = args.get('path_key', args.get('file_path', ''))
                    is_filename_only = args.get('is_filename_only', False)
                    
                    # åªä¸ºçœŸæ­£éœ€è¦å¤„ç†çš„æ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
                    if Path(file_path) in files_to_process_full or Path(file_path) in files_to_process_filename_only:
                        processed_count += 1
                        file_name = args.get('display_name', args.get('path_key', 'unknown'))
                        detail = f"æ­£åœ¨å¤„ç†: {file_name}"
                        
                        progress.update({
                            'stage': 'extracting',
                            'current': processed_count,
                            'total': real_processing_total,
                            'message': detail
                        })
                        yield progress
                    
                    # å¤„ç†å•ä¸ªæ–‡ä»¶
                    result = _extract_worker(args)
                    extraction_results.append(result)
                    
                    # --- ADDED: åœ¨æ–‡ä»¶å¤„ç†å®Œæˆåæ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
                    if cancel_callback and cancel_callback():
                        print(f"ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œå·²å¤„ç† {i+1} ä¸ªæ–‡ä»¶ï¼Œåœæ­¢å¤„ç†å‰©ä½™æ–‡ä»¶")
                        raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                    # ----------------------------------------
                    
                except InterruptedError:
                    # --- ADDED: ä¸“é—¨å¤„ç†å–æ¶ˆå¼‚å¸¸ ---
                    print(f"æ–‡ä»¶å¤„ç†è¢«å–æ¶ˆï¼Œå·²å¤„ç† {i} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(worker_args_list) - i} ä¸ªæ–‡ä»¶æœªå¤„ç†")
                    # ç›´æ¥é‡æ–°æŠ›å‡ºï¼Œä¸æ·»åŠ åˆ°ç»“æœä¸­
                    raise
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    # åˆ›å»ºé”™è¯¯ç»“æœ
                    error_result = {
                        'path_key': args.get('path_key', 'unknown'),
                        'display_name': args.get('display_name', 'unknown'),
                        'text_content': '',
                        'structure': [],
                        'error': str(e),
                        'mtime': args.get('original_mtime', 0),
                        'fsize': args.get('original_fsize', 0),
                        'file_type': '',
                        'filename': '',
                        'ocr_enabled_for_file': False,
                        'content_truncated': False
                    }
                    extraction_results.append(error_result)
                    
                    # é”™è¯¯æ—¶ä¹Ÿå‘é€è¿›åº¦æ›´æ–°
                    current_file = i + 1
                    file_name = args.get('display_name', args.get('path_key', 'unknown'))
                    detail = f"å¤„ç†å¤±è´¥: {file_name} - {str(e)}"
                    
                    progress.update({
                        'stage': 'extracting',
                        'current': current_file,
                        'total': total_files,
                        'message': detail
                    })
                    yield progress

            # --- MODIFIED: åœ¨æå–å®Œæˆåæ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ ---
            if cancel_callback and cancel_callback():
                print("ç”¨æˆ·åœ¨æ–‡ä»¶æå–å®Œæˆåå–æ¶ˆæ“ä½œ")
                raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
            # ----------------------------------------

            progress.update({
                'stage': 'indexing',
                'message': 'å¼€å§‹ç´¢å¼•æ–‡æ¡£...'
            })
            yield progress

            # 6. æ‰¹é‡ç´¢å¼•æ–‡æ¡£
            with ix.writer() as writer:
                # æ ¹æ®preserve_removed_dirså‚æ•°å†³å®šæ˜¯å¦åˆ é™¤æ–‡ä»¶
                if incremental and 'deleted_files' in locals() and not preserve_removed_dirs:
                    print(f"ç‰©ç†åˆ é™¤ {len(deleted_files)} ä¸ªç´¢å¼•æ¡ç›®ï¼ˆpreserve_removed_dirs=Falseï¼‰")
                    remove_deleted_files_from_index(writer, deleted_files)
                elif incremental and 'deleted_files' in locals() and preserve_removed_dirs:
                    print(f"ä¿ç•™ {len(deleted_files)} ä¸ªå·²ç§»é™¤ç›®å½•çš„ç´¢å¼•æ¡ç›®ï¼ˆpreserve_removed_dirs=Trueï¼‰")
                    print("æœç´¢æ—¶å°†é€šè¿‡ç›®å½•è¿‡æ»¤æ’é™¤è¿™äº›ç»“æœ")

                # ç´¢å¼•æå–çš„æ–‡æ¡£
                success_count = 0
                error_count = 0
                total_results = len(extraction_results)

                for i, result in enumerate(extraction_results):
                    # --- MODIFIED: åœ¨æ¯ä¸ªæ–‡æ¡£ç´¢å¼•å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ ---
                    if cancel_callback and cancel_callback():
                        print(f"ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œåœæ­¢ç´¢å¼•å‰©ä½™ {len(extraction_results) - i} ä¸ªæ–‡æ¡£")
                        raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                    # ------------------------------------------------
                        
                    try:
                        if result.get('error'):
                            # è®°å½•é”™è¯¯æ–‡ä»¶
                            record_skipped_file(index_dir_path, result['path_key'], result['error'])
                            error_count += 1
                            
                            # å‘é€è¿›åº¦æ›´æ–°
                            current = i + 1
                            detail = f"è·³è¿‡é”™è¯¯æ–‡ä»¶: {result.get('display_name', result['path_key'])}"
                            progress.update({
                                'stage': 'indexing',
                                'current': current,
                                'total': total_results,
                                'message': detail
                            })
                            yield progress
                            continue

                        # ç´¢å¼•æ–‡æ¡£
                        writer.add_document(
                            path=result['path_key'],
                            content=result['text_content'],
                            filename_text=result['filename'],
                            structure_map=json.dumps(result['structure'], ensure_ascii=False),
                            last_modified=result['mtime'],
                            file_size=result['fsize'],
                            file_type=result['file_type'],
                            indexed_with_ocr=result['ocr_enabled_for_file']
                        )
                        success_count += 1
                        
                        # å‘é€è¿›åº¦æ›´æ–°
                        current = i + 1
                        detail = f"å·²ç´¢å¼•: {result.get('display_name', result['path_key'])}"
                        progress.update({
                            'stage': 'indexing',
                            'current': current,
                            'total': total_results,
                            'message': detail
                        })
                        yield progress
                        
                        # --- ADDED: åœ¨ç´¢å¼•æ–‡æ¡£åæ£€æŸ¥å–æ¶ˆçŠ¶æ€ ---
                        if cancel_callback and cancel_callback():
                            print(f"ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œå·²ç´¢å¼• {i+1} ä¸ªæ–‡æ¡£ï¼Œåœæ­¢ç´¢å¼•å‰©ä½™æ–‡æ¡£")
                            raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                        # ----------------------------------------
                        
                    except InterruptedError:
                        # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                        raise
                    except Exception as e:
                        error_count += 1
                        record_skipped_file(index_dir_path, result.get('path_key', 'unknown'), f"ç´¢å¼•é”™è¯¯: {e}")
                        print(f"ç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™: {e}")
                        
                        # å‘é€è¿›åº¦æ›´æ–°
                        current = i + 1
                        detail = f"ç´¢å¼•å¤±è´¥: {result.get('display_name', result.get('path_key', 'unknown'))} - {str(e)}"
                        progress.update({
                            'stage': 'indexing',
                            'current': current,
                            'total': total_results,
                            'message': detail
                        })
                        yield progress

                progress.update({
                    'files_processed': success_count,
                    'errors': error_count
                })

        # 7. æ›´æ–°æ–‡ä»¶ç¼“å­˜
        if incremental:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
            if cancel_callback and cancel_callback():
                raise InterruptedError("æ“ä½œè¢«ç”¨æˆ·å–æ¶ˆ")
                
            progress.update({
                'stage': 'updating_cache',
                'message': 'æ›´æ–°æ–‡ä»¶ç¼“å­˜...'
            })
            yield progress

            # æ›´æ–°ç¼“å­˜ï¼ˆä½¿ç”¨æ–°çš„ç¼“å­˜æ¡ç›®æ ¼å¼ï¼ŒåŒ…å«hashå’Œmodeï¼‰
            all_processed_files = set()  # è·Ÿè¸ªå·²å¤„ç†çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤
            
            for file_path in all_files:
                path_str = normalize_path_for_index(str(file_path))
                if path_str not in all_processed_files:
                    file_cache[path_str] = get_file_cache_entry(file_path, "full")
                    all_processed_files.add(path_str)
                
            for file_path in filename_only_files:
                path_str = normalize_path_for_index(str(file_path))
                if path_str not in all_processed_files:
                    file_cache[path_str] = get_file_cache_entry(file_path, "filename_only")
                    all_processed_files.add(path_str)

            save_file_index_cache(index_dir_path, file_cache)

        # 8. è®°å½•è·³è¿‡çš„æ–‡ä»¶
        for skip_info in skipped_files:
            if isinstance(skip_info, dict):
                # æ–°æ ¼å¼ï¼šå­—å…¸åŒ…å«è¯¦ç»†ä¿¡æ¯
                record_skipped_file(index_dir_path, skip_info['path'], skip_info['reason'])
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼šå…ƒç»„ (file_path, reason)
                file_path, reason = skip_info
                record_skipped_file(index_dir_path, str(file_path), reason)

        # å®Œæˆ
        progress.update({
            'stage': 'complete',
            'message': f'ç´¢å¼•å®Œæˆï¼å¤„ç†äº† {progress["files_processed"]} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {progress["files_skipped"]} ä¸ªæ–‡ä»¶ï¼Œ{progress["errors"]} ä¸ªé”™è¯¯'
        })
        yield progress

    except InterruptedError:
        # ç”¨æˆ·å–æ¶ˆæ“ä½œ
        progress.update({
            'stage': 'cancelled',
            'message': 'ç´¢å¼•æ“ä½œå·²è¢«ç”¨æˆ·å–æ¶ˆ'
        })
        yield progress
        raise
    except Exception as e:
        progress.update({
            'stage': 'error',
            'message': f'ç´¢å¼•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}'
        })
        yield progress
        raise

# --- ç»“æŸç´¢å¼•ä¼˜åŒ–å‡½æ•° ---

# --- é«˜çº§ç´¢å¼•ä¼˜åŒ–åŠŸèƒ½ ---

def get_file_hash(file_path: Path, index_mode: str = "full") -> str:
    """
    è·å–æ–‡ä»¶çš„ç®€å•å“ˆå¸Œå€¼ï¼ˆåŸºäºä¿®æ”¹æ—¶é—´å’Œå¤§å°ï¼Œä¸åŒ…å«ç´¢å¼•æ¨¡å¼ï¼‰

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        index_mode: ç´¢å¼•æ¨¡å¼ï¼ˆ"full" æˆ– "filename_only"ï¼‰- ä¿ç•™å‚æ•°å…¼å®¹æ€§ä½†ä¸ç”¨äºå“ˆå¸Œè®¡ç®—

    Returns:
        str: æ–‡ä»¶å“ˆå¸Œå€¼
    """
    try:
        stat = file_path.stat()
        # ä½¿ç”¨æ•´æ•°ç²¾åº¦çš„ä¿®æ”¹æ—¶é—´å’Œæ–‡ä»¶å¤§å°ç”Ÿæˆå“ˆå¸Œ
        # ä¸åŒ…å«ç´¢å¼•æ¨¡å¼ï¼Œé¿å…ç´¢å¼•æ¨¡å¼å˜æ›´æ—¶è¯¯åˆ¤ä¸ºæ–‡ä»¶å˜æ›´
        mtime_int = int(stat.st_mtime)  # ä½¿ç”¨æ•´æ•°ç²¾åº¦é¿å…æµ®ç‚¹è¯¯å·®
        hash_str = f"{mtime_int}_{stat.st_size}"
        return hash_str
    except Exception:
        return "unknown"

def get_file_cache_entry(file_path: Path, index_mode: str) -> dict:
    """
    è·å–æ–‡ä»¶çš„ç¼“å­˜æ¡ç›®ï¼ˆåŒ…å«å“ˆå¸Œå’Œç´¢å¼•æ¨¡å¼ï¼‰

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        index_mode: ç´¢å¼•æ¨¡å¼ï¼ˆ"full" æˆ– "filename_only"ï¼‰

    Returns:
        dict: åŒ…å«hashå’Œmodeçš„ç¼“å­˜æ¡ç›®
    """
    return {
        "hash": get_file_hash(file_path),
        "mode": index_mode
    }

def load_file_index_cache(index_dir_path: str) -> dict:
    """
    åŠ è½½æ–‡ä»¶ç´¢å¼•ç¼“å­˜ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜æ›´

    Args:
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„

    Returns:
        dict: æ–‡ä»¶è·¯å¾„åˆ°å“ˆå¸Œå€¼çš„æ˜ å°„
    """
    cache_file = Path(index_dir_path) / "file_cache.json"
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")
    return {}

def save_file_index_cache(index_dir_path: str, cache: dict):
    """
    ä¿å­˜æ–‡ä»¶ç´¢å¼•ç¼“å­˜

    Args:
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        cache: æ–‡ä»¶ç¼“å­˜å­—å…¸
    """
    cache_file = Path(index_dir_path) / "file_cache.json"
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

def detect_file_changes(files: list[Path], cache: dict, filename_only_files: list[Path] = None) -> tuple[list[Path], list[Path], list[str]]:
    """
    æ£€æµ‹æ–‡ä»¶å˜æ›´ï¼ˆæ”¯æŒç´¢å¼•æ¨¡å¼æ„ŸçŸ¥ï¼‰

    Args:
        files: å½“å‰å®Œæ•´ç´¢å¼•æ–‡ä»¶åˆ—è¡¨
        cache: ç°æœ‰æ–‡ä»¶ç¼“å­˜
        filename_only_files: ä»…æ–‡ä»¶åç´¢å¼•æ–‡ä»¶åˆ—è¡¨

    Returns:
        tuple[list[Path], list[Path], list[str]]: (æ–°æ–‡ä»¶, ä¿®æ”¹çš„æ–‡ä»¶, åˆ é™¤çš„æ–‡ä»¶è·¯å¾„)
    """
    current_files = {}
    new_files = []
    modified_files = []
    
    if filename_only_files is None:
        filename_only_files = []

    # æ£€æŸ¥å®Œæ•´ç´¢å¼•æ–‡ä»¶
    for file_path in files:
        path_str = normalize_path_for_index(str(file_path))
        current_entry = get_file_cache_entry(file_path, "full")
        current_files[path_str] = current_entry

        if path_str not in cache:
            new_files.append(file_path)
            print(f"æ–°æ–‡ä»¶: {file_path.name} (å…¨æ–‡ç´¢å¼•)")
        else:
            cached_entry = cache[path_str]
            # å…¼å®¹æ—§ç¼“å­˜æ ¼å¼ï¼ˆçº¯å­—ç¬¦ä¸²ï¼‰
            if isinstance(cached_entry, str):
                cached_hash = cached_entry
                cached_mode = "unknown"
            else:
                cached_hash = cached_entry.get("hash", "")
                cached_mode = cached_entry.get("mode", "unknown")
            
            current_hash = current_entry["hash"]
            current_mode = current_entry["mode"]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¤„ç†
            file_changed = cached_hash != current_hash
            mode_upgraded = cached_mode == "filename_only" and current_mode == "full"
            
            if file_changed or mode_upgraded:
                modified_files.append(file_path)
                if file_changed:
                    print(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {file_path.name} (æ–‡ä»¶æ—¶é—´æˆ–å¤§å°å‘ç”Ÿå˜åŒ–)")
                if mode_upgraded:
                    print(f"æ£€æµ‹åˆ°ç´¢å¼•æ¨¡å¼å‡çº§: {file_path.name} (ä»ä»…æ–‡ä»¶åå‡çº§åˆ°å…¨æ–‡ç´¢å¼•)")

    # æ£€æŸ¥ä»…æ–‡ä»¶åç´¢å¼•æ–‡ä»¶
    for file_path in filename_only_files:
        path_str = normalize_path_for_index(str(file_path))
        current_entry = get_file_cache_entry(file_path, "filename_only")
        current_files[path_str] = current_entry

        if path_str not in cache:
            new_files.append(file_path)
            print(f"æ–°æ–‡ä»¶: {file_path.name} (ä»…æ–‡ä»¶åç´¢å¼•)")
        else:
            cached_entry = cache[path_str]
            # å…¼å®¹æ—§ç¼“å­˜æ ¼å¼ï¼ˆçº¯å­—ç¬¦ä¸²ï¼‰
            if isinstance(cached_entry, str):
                cached_hash = cached_entry
                cached_mode = "unknown"
            else:
                cached_hash = cached_entry.get("hash", "")
                cached_mode = cached_entry.get("mode", "unknown")
            
            current_hash = current_entry["hash"]
            current_mode = current_entry["mode"]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¤„ç†
            file_changed = cached_hash != current_hash
            # æ³¨æ„ï¼šä»fullé™çº§åˆ°filename_onlyä¸éœ€è¦é‡æ–°å¤„ç†
            
            if file_changed:
                modified_files.append(file_path)
                print(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {file_path.name} (æ–‡ä»¶æ—¶é—´æˆ–å¤§å°å‘ç”Ÿå˜åŒ–)")

    # CRITICAL FIX REMOVED: è¿™æ®µä»£ç å¯¼è‡´æ‰€æœ‰ç¼“å­˜æ–‡ä»¶éƒ½è¢«é‡æ–°å¤„ç†
    # å¢é‡ç´¢å¼•åº”è¯¥åªåŸºäºæ–‡ä»¶æœ¬èº«çš„å˜æ›´ï¼Œè€Œä¸æ˜¯é…ç½®å˜æ›´
    # å¦‚æœç”¨æˆ·æ–°å¢æ–‡ä»¶ç±»å‹ï¼Œscan_documents_optimizedå·²ç»ä¼šæ­£ç¡®å‘ç°è¿™äº›æ–‡ä»¶
    # å¹¶ä¸”å®ƒä»¬ä¸åœ¨ç¼“å­˜ä¸­ï¼Œæ‰€ä»¥ä¼šè¢«æ­£ç¡®æ ‡è®°ä¸ºnew_files

    # æ£€æŸ¥åˆ é™¤çš„æ–‡ä»¶
    deleted_files = [path for path in cache.keys() if path not in current_files]

    return new_files, modified_files, deleted_files

def prepare_worker_arguments_batch(files: list[Path], enable_ocr: bool, extraction_timeout: int,
                                 content_limit_kb: int, index_dir_path: str, filename_only_files: list[Path] = None, cancel_callback=None) -> list[dict]:
    """
    æ‰¹é‡å‡†å¤‡å·¥ä½œè¿›ç¨‹å‚æ•°

    Args:
        files: éœ€è¦å®Œæ•´ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨
        enable_ocr: æ˜¯å¦å¯ç”¨OCR
        extraction_timeout: æå–è¶…æ—¶æ—¶é—´
        content_limit_kb: å†…å®¹å¤§å°é™åˆ¶ï¼ˆKBï¼‰
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        filename_only_files: ä»…ç´¢å¼•æ–‡ä»¶åçš„æ–‡ä»¶åˆ—è¡¨
        cancel_callback: å–æ¶ˆæ£€æŸ¥å›è°ƒå‡½æ•°

    Returns:
        list[dict]: å·¥ä½œè¿›ç¨‹å‚æ•°åˆ—è¡¨
    """
    worker_args_list = []
    content_limit_bytes = content_limit_kb * 1024 if content_limit_kb > 0 else 0

    # å¤„ç†éœ€è¦å®Œæ•´ç´¢å¼•çš„æ–‡ä»¶
    for file_path in files:
        # åŠ¨æ€è®¾ç½®PDF OCRè¶…æ—¶
        actual_timeout = extraction_timeout
        if file_path.suffix.lower() == '.pdf' and enable_ocr:
            try:
                file_size = file_path.stat().st_size
                # æ ¹æ®PDFæ–‡ä»¶å¤§å°è®¾ç½®æ›´åˆç†çš„è¶…æ—¶æ—¶é—´
                if file_size < 5 * 1024 * 1024:  # å°äº5MB
                    actual_timeout = min(60, extraction_timeout)
                elif file_size < 20 * 1024 * 1024:  # 5-20MB
                    actual_timeout = min(180, extraction_timeout)
                elif file_size < 50 * 1024 * 1024:  # 20-50MB
                    actual_timeout = min(300, extraction_timeout)
                else:  # å¤§äº50MB
                    actual_timeout = extraction_timeout

                print(f"PDFæ–‡ä»¶ {file_path.name} ({file_size // (1024*1024)}MB) è®¾ç½®OCRè¶…æ—¶: {actual_timeout}ç§’")
            except Exception:
                pass

        file_stat = file_path.stat()

        worker_args = {
            'path_key': str(file_path),
            'file_type': 'file',
            'enable_ocr': enable_ocr and file_path.suffix.lower() == '.pdf',
            'extraction_timeout': actual_timeout,
            'content_limit_bytes': content_limit_bytes,
            'index_dir_path': index_dir_path,
            'original_mtime': file_stat.st_mtime,
            'original_fsize': file_stat.st_size,
            'display_name': file_path.name,
            'cancel_callback': cancel_callback,  # æ·»åŠ å–æ¶ˆå›è°ƒ
            'is_filename_only': False  # æ ‡è®°ä¸ºå®Œæ•´ç´¢å¼•
        }

        worker_args_list.append(worker_args)

    # å¤„ç†ä»…ç´¢å¼•æ–‡ä»¶åçš„æ–‡ä»¶
    if filename_only_files:
        for file_path in filename_only_files:
            file_stat = file_path.stat()
            
            worker_args = {
                'path_key': str(file_path),
                'file_type': 'file',
                'enable_ocr': False,  # ä»…æ–‡ä»¶åç´¢å¼•ä¸éœ€è¦OCR
                'extraction_timeout': 1,  # å¾ˆçŸ­çš„è¶…æ—¶ï¼Œå› ä¸ºä¸éœ€è¦æå–å†…å®¹
                'content_limit_bytes': 0,  # ä¸é™åˆ¶å†…å®¹ï¼Œå› ä¸ºä¸æå–
                'index_dir_path': index_dir_path,
                'original_mtime': file_stat.st_mtime,
                'original_fsize': file_stat.st_size,
                'display_name': file_path.name,
                'cancel_callback': cancel_callback,
                'is_filename_only': True  # æ ‡è®°ä¸ºä»…æ–‡ä»¶åç´¢å¼•
            }
            
            worker_args_list.append(worker_args)

    return worker_args_list

def process_files_multiprocess(worker_args_list: list[dict], max_workers: int = None, progress_callback=None) -> list[dict]:
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        worker_args_list: å·¥ä½œè¿›ç¨‹å‚æ•°åˆ—è¡¨
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total, detail)å‚æ•°

    Returns:
        list[dict]: å¤„ç†ç»“æœåˆ—è¡¨
    """
    if max_workers is None:
        max_workers = get_optimal_worker_count("io_intensive")

    results = []
    total_files = len(worker_args_list)

    # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–çš„å¤šè¿›ç¨‹å¤„ç†
    # åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œè¿™åº”è¯¥ä½¿ç”¨çœŸæ­£çš„multiprocessing.Pool
    print(f"å¼€å§‹å¤šè¿›ç¨‹å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹")

    for i, args in enumerate(worker_args_list):
        try:
            # åœ¨å®é™…å®ç°ä¸­è¿™é‡Œä¼šè°ƒç”¨çœŸæ­£çš„å¤šè¿›ç¨‹å¤„ç†
            result = _extract_worker(args)
            results.append(result)

            # å®æ—¶è¿›åº¦æ›´æ–°
            current_file = i + 1
            file_name = args.get('display_name', args.get('path_key', 'unknown'))
            detail = f"æ­£åœ¨å¤„ç†: {file_name}"
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(current_file, total_files, detail)

            # ç®€åŒ–çš„æ§åˆ¶å°è¿›åº¦æŠ¥å‘Šï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
            if current_file % 10 == 0:
                print(f"å·²å¤„ç† {current_file}/{total_files} ä¸ªæ–‡ä»¶")

        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            # åˆ›å»ºé”™è¯¯ç»“æœ
            error_result = {
                'path_key': args.get('path_key', 'unknown'),
                'display_name': args.get('display_name', 'unknown'),
                'text_content': '',
                'structure': [],
                'error': str(e),
                'mtime': args.get('original_mtime', 0),
                'fsize': args.get('original_fsize', 0),
                'file_type': '',
                'filename': '',
                'ocr_enabled_for_file': False,
                'content_truncated': False
            }
            results.append(error_result)
            
            # é”™è¯¯æ—¶ä¹Ÿæ›´æ–°è¿›åº¦
            if progress_callback:
                current_file = i + 1
                file_name = args.get('display_name', args.get('path_key', 'unknown'))
                detail = f"å¤„ç†å¤±è´¥: {file_name} - {str(e)}"
                progress_callback(current_file, total_files, detail)

    return results

def batch_index_documents(writer, extraction_results: list[dict], index_dir_path: str, progress_callback=None) -> tuple[int, int]:
    """
    æ‰¹é‡ç´¢å¼•æ–‡æ¡£ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        writer: Whoosh writer
        extraction_results: æå–ç»“æœåˆ—è¡¨
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total, detail)å‚æ•°

    Returns:
        tuple[int, int]: (æˆåŠŸç´¢å¼•æ•°, é”™è¯¯æ•°)
    """
    success_count = 0
    error_count = 0
    total_results = len(extraction_results)

    for i, result in enumerate(extraction_results):
        try:
            if result.get('error'):
                # è®°å½•é”™è¯¯æ–‡ä»¶
                record_skipped_file(index_dir_path, result['path_key'], result['error'])
                error_count += 1
                
                # å‘é€è¿›åº¦æ›´æ–°
                if progress_callback:
                    current = i + 1
                    detail = f"è·³è¿‡é”™è¯¯æ–‡ä»¶: {result.get('display_name', result['path_key'])}"
                    progress_callback(current, total_results, detail)
                continue

            content = result.get('text_content', '')
            structure = result.get('structure', [])

            if not content and not structure:
                # ç©ºå†…å®¹ï¼Œè·³è¿‡
                record_skipped_file(index_dir_path, result['path_key'], "æå–çš„å†…å®¹ä¸ºç©º")
                error_count += 1
                
                # å‘é€è¿›åº¦æ›´æ–°
                if progress_callback:
                    current = i + 1
                    detail = f"è·³è¿‡ç©ºå†…å®¹æ–‡ä»¶: {result.get('display_name', result['path_key'])}"
                    progress_callback(current, total_results, detail)
                continue

            # è·å–æ–‡ä»¶ä¿¡æ¯
            path_key = result['path_key']
            file_path = Path(path_key)

            # æ·»åŠ åˆ°ç´¢å¼•
            writer.update_document(
                path=normalize_path_for_index(path_key),
                content=content,
                filename_text=file_path.name,
                structure_map=json.dumps(structure),
                last_modified=result['mtime'],
                file_size=result['fsize'],
                file_type=result.get('file_type', '').lstrip('.'),
                indexed_with_ocr=result.get('ocr_enabled_for_file', False)
            )

            success_count += 1
            
            # å‘é€è¿›åº¦æ›´æ–°
            if progress_callback:
                current = i + 1
                detail = f"å·²ç´¢å¼•: {result.get('display_name', file_path.name)}"
                progress_callback(current, total_results, detail)

        except Exception as e:
            error_count += 1
            record_skipped_file(index_dir_path, result.get('path_key', 'unknown'), f"ç´¢å¼•é”™è¯¯: {e}")
            print(f"ç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            
            # å‘é€è¿›åº¦æ›´æ–°
            if progress_callback:
                current = i + 1
                detail = f"ç´¢å¼•å¤±è´¥: {result.get('display_name', result.get('path_key', 'unknown'))} - {str(e)}"
                progress_callback(current, total_results, detail)

    return success_count, error_count

def remove_deleted_files_from_index(writer, deleted_files: list[str]):
    """
    ä»ç´¢å¼•ä¸­åˆ é™¤å·²åˆ é™¤çš„æ–‡ä»¶

    Args:
        writer: Whoosh writer
        deleted_files: å·²åˆ é™¤çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    for file_path in deleted_files:
        try:
            writer.delete_by_term('path', normalize_path_for_index(file_path))
            print(f"ä»ç´¢å¼•ä¸­åˆ é™¤: {file_path}")
        except Exception as e:
            print(f"åˆ é™¤ç´¢å¼•é¡¹æ—¶å‡ºé”™ {file_path}: {e}")

# --- ç»“æŸé«˜çº§ç´¢å¼•ä¼˜åŒ–åŠŸèƒ½ ---

# --- å…¼å®¹æ€§åŒ…è£…å‡½æ•° ---

def create_or_update_index_legacy(source_directories, index_dir_path, enable_ocr, 
                                 extraction_timeout=300, txt_content_limit_kb=1024, 
                                 file_types_to_index=None, filename_only_types=None, 
                                 cancel_callback=None, preserve_removed_dirs=True):
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œä¿æŒä¸ç°æœ‰GUIçš„å…¼å®¹æ€§
    å°†æ—§ç‰ˆæœ¬çš„å‚æ•°æ˜ å°„åˆ°æ–°çš„ä¼˜åŒ–ç‰ˆæœ¬

    Args:
        source_directories: æºç›®å½•åˆ—è¡¨
        index_dir_path: ç´¢å¼•ç›®å½•è·¯å¾„
        enable_ocr: æ˜¯å¦å¯ç”¨OCR
        extraction_timeout: æå–è¶…æ—¶æ—¶é—´
        txt_content_limit_kb: TXTå†…å®¹é™åˆ¶ï¼ˆKBï¼‰
        file_types_to_index: è¦ç´¢å¼•çš„æ–‡ä»¶ç±»å‹åˆ—è¡¨
        cancel_callback: å–æ¶ˆæ£€æŸ¥å›è°ƒå‡½æ•°

    Yields:
        dict: è¿›åº¦ä¿¡æ¯ï¼ˆè½¬æ¢ä¸ºæ—§æ ¼å¼ï¼‰
    """
    print("ä½¿ç”¨å…¼å®¹æ€§åŒ…è£…å‡½æ•°è°ƒç”¨ä¼˜åŒ–ç‰ˆç´¢å¼•...")

    # å°†æ–°çš„ä¼˜åŒ–å‚æ•°æ˜ å°„åˆ°æ—§çš„æ ¼å¼
    try:
        # è°ƒç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„ç´¢å¼•å‡½æ•°
        for progress in create_or_update_index(
            directories=source_directories,
            index_dir_path=index_dir_path,
            enable_ocr=enable_ocr,
            extraction_timeout=extraction_timeout,
            content_limit_kb=txt_content_limit_kb,
            max_file_size_mb=100,  # é»˜è®¤å€¼
            skip_system_files=True,  # é»˜è®¤å¯ç”¨
            incremental=True,  # é»˜è®¤å¯ç”¨å¢é‡ç´¢å¼•
            max_workers=None,  # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
            cancel_callback=cancel_callback,  # ä¼ é€’å–æ¶ˆå›è°ƒ
            file_types_to_index=file_types_to_index,  # ä¼ é€’å®Œæ•´ç´¢å¼•æ–‡ä»¶ç±»å‹
            filename_only_types=filename_only_types,  # æ–°å¢ï¼šä¼ é€’ä»…æ–‡ä»¶åç´¢å¼•æ–‡ä»¶ç±»å‹
            preserve_removed_dirs=preserve_removed_dirs  # æ–°å¢ï¼šä¼ é€’ç›®å½•ä¿ç•™å‚æ•°
        ):
            # å°†æ–°æ ¼å¼çš„è¿›åº¦ä¿¡æ¯è½¬æ¢ä¸ºæ—§æ ¼å¼
            old_format_progress = convert_progress_to_legacy_format(progress)
            yield old_format_progress

    except InterruptedError:
        # å¤„ç†ç”¨æˆ·å–æ¶ˆ
        cancelled_progress = {
            'type': 'complete',
            'message': 'ç´¢å¼•å·²è¢«ç”¨æˆ·å–æ¶ˆ',
            'summary': {
                'message': 'ç´¢å¼•å·²è¢«ç”¨æˆ·å–æ¶ˆã€‚',
                'added': 0,
                'updated': 0,
                'deleted': 0,
                'errors': 0,
                'cancelled': True
            }
        }
        yield cancelled_progress
        raise
    except Exception as e:
        # å¦‚æœä¼˜åŒ–ç‰ˆæœ¬å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶æŠ›å‡º
        print(f"ä¼˜åŒ–ç‰ˆç´¢å¼•å¤±è´¥: {e}")
        raise

def convert_progress_to_legacy_format(new_progress):
    """
    å°†æ–°æ ¼å¼çš„è¿›åº¦ä¿¡æ¯è½¬æ¢ä¸ºæ—§æ ¼å¼ï¼Œç¡®ä¿GUIå…¼å®¹æ€§

    Args:
        new_progress: æ–°æ ¼å¼çš„è¿›åº¦å­—å…¸

    Returns:
        dict: æ—§æ ¼å¼çš„è¿›åº¦å­—å…¸ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
    """
    # --- ENHANCED: æ›´å¼ºçš„ç±»å‹æ£€æŸ¥å’Œé”™è¯¯å¤„ç† ---
    if not isinstance(new_progress, dict):
        print(f"WARNING: convert_progress_to_legacy_format received non-dict: {type(new_progress)}")
        return {
            'type': 'error',
            'message': f'è¿›åº¦æ ¼å¼é”™è¯¯: {type(new_progress)}',
            'current': 0,
            'total': 0,
            'phase': 'error',
            'detail': 'è¿›åº¦æ•°æ®æ ¼å¼é”™è¯¯'
        }
    
    stage = new_progress.get('stage', '')
    message = new_progress.get('message', '')

    # æ ¹æ®é˜¶æ®µæ˜ å°„åˆ°æ—§çš„æ¶ˆæ¯ç±»å‹
    if stage == 'scanning':
        return {
            'type': 'status',
            'message': message,
            'phase': 'scanning'
        }
    elif stage == 'scanning_complete':
        return {
            'type': 'status',
            'message': message,
            'phase': 'scan_complete'
        }
    elif stage == 'change_detection':
        return {
            'type': 'status',
            'message': message,
            'phase': 'change_detection'
        }
    elif stage == 'change_detection_complete':
        return {
            'type': 'status',
            'message': message,
            'phase': 'change_detection_complete'
        }
    elif stage == 'processing_start':
        return {
            'type': 'status',
            'message': message,
            'phase': 'processing'
        }
    elif stage == 'extracting':
        return {
            'type': 'progress',
            'current': new_progress.get('current', 0),
            'total': new_progress.get('total', 0),
            'phase': 'extracting',
            'detail': message
        }
    elif stage == 'indexing':
        return {
            'type': 'progress',
            'current': new_progress.get('current', 0),
            'total': new_progress.get('total', 0),
            'phase': 'indexing',
            'detail': message
        }
    elif stage == 'updating_cache':
        return {
            'type': 'status',
            'message': message,
            'phase': 'updating_cache'
        }
    elif stage == 'complete':
        # æ„å»ºå®Œæˆæ‘˜è¦
        files_processed = new_progress.get('files_processed', 0)
        files_skipped = new_progress.get('files_skipped', 0)
        errors = new_progress.get('errors', 0)

        summary = {
            'message': message,
            'added': files_processed,  # ç®€åŒ–ï¼šå°†å¤„ç†çš„æ–‡ä»¶æ•°ä½œä¸ºæ·»åŠ æ•°
            'updated': 0,  # æ–°ç‰ˆæœ¬ä¸åŒºåˆ†æ·»åŠ å’Œæ›´æ–°
            'deleted': 0,  # åˆ é™¤æ•°æš‚æ—¶è®¾ä¸º0
            'errors': errors,
            'cancelled': False
        }

        return {
            'type': 'complete',
            'message': message,
            'summary': summary
        }
    elif stage == 'cancelled':
        # å¤„ç†å–æ¶ˆçŠ¶æ€
        summary = {
            'message': message,
            'added': 0,
            'updated': 0,
            'deleted': 0,
            'errors': 0,
            'cancelled': True
        }

        return {
            'type': 'complete',
            'message': message,
            'summary': summary
        }
    elif stage == 'error':
        return {
            'type': 'error',
            'message': message
        }
    else:
        # é»˜è®¤æƒ…å†µ
        return {
            'type': 'status',
            'message': message,
            'phase': stage
        }

# --- ç»“æŸå…¼å®¹æ€§åŒ…è£…å‡½æ•° ---

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œæµ‹è¯•
    print("æ‰§è¡Œç´¢å¼•ä¼˜åŒ–æµ‹è¯•...")
    # test_optimized_indexing()  # æš‚æ—¶æ³¨é‡Šæ‰æµ‹è¯•å‡½æ•°è°ƒç”¨
